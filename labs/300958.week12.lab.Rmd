% Assessing for Impact
% 300958 Social Web Analysis 
% Week 12 Lab

```{r setup, include=FALSE}
opts_chunk$set(fig.path='figure/week12-lab-')
```

`r opts_chunk$set(prompt=TRUE, comment=NA, fig.cap="")`


# Difference in means

We saw in the lecture that we are able to test for a difference in
means by shuffling the values between samples, when keeping the the
sample sizes the same.

Using the following before and after data:

```{r echo=FALSE}
options(scipen=1, digits=2)
suppressWarnings(require(xtable, quietly=TRUE))
set.seed(56223)
sim.one = function(mb, ma, n) {
before = rpois(n, mb)
after = rpois(n, ma)
cat("before = ")
dput(before, control=NULL)
cat("after = ")
dput(after, control=NULL)
}
sim.one(140,175,5)
```

write the function `shuffleMean` to:

1. take the two samples
2. shuffle the items betwen the groups
3. compute the difference in means of the two groups

Then compute view the randomisation distribution using:
```{r eval = FALSE}
randDist = replicate(1000, shuffleMean(before, after))
hist(randDist)
```

To perform shuffling, we can combine the two vectors, sample a set of
positions, then split the vector into the items at those positions,
and the items not at those positions. E.g.
```{r eval=FALSE}
combined = c(before, after)
chosen = sample(length(combined), size = length(before), replace = FALSE)
shuffledBefore = combined[chosen]
shuffledAfter = combined[-chosen]
```


Complete the test by:

1. computing the sample difference in means of the data.
2. computng the $p$ value using the sample difference in means and `randDist`. 

Was there evidence of a difference in population means?

Perform the same test on the following data.

```{r echo=FALSE}
sim.one(220, 240, 4)
sim.one(100,197, 6)
```


Finaly compare the $p$ value to the $p$ value produced when using:
```{r eval = FALSE}
t.test(before, after)
```
The above test assumes that either the data is Normal, or the sample size is large.
The $p$ values should be similar to those you have computed. Use this to check your answers.



# BACI designs 


We also saw in the lecture that we can examine the impact of an event
when using a control.

We have obtained tweet counts from before and after and event from the
impacted company and a control company.

```{r echo=FALSE}
set.seed(86755)
sim.two = function(BIm, AIm, BCm, ACm, n) {
BIx = rpois(n, 96)
AIx = rpois(n, 129)
BCx = rpois(n, 52)
ACx = rpois(n, 85)
cat("before.control = ")
dput(BCx, control=NULL)
cat("after.control = ")
dput(ACx, control=NULL)
cat("before.impact = ")
dput(BIx, control=NULL)
cat("after.impact = ")
dput(AIx, control=NULL)
}
sim.two(96,129,52,85, 4)
```

We want to test if the interaction term $\gamma = 0$.

## Shaping the data

To perform the analysis, we need to shuffle to data. In order to
shuffle the data, we must first convert the data to a data frame.

Create the data frame `d` with three columns:

- the first column called `company`, containing either `impact` or `control`.
- the second column called `when`, containing either `before` or `after`.
- the third column called `count`, containing the tweet counts

Since there are 16 tweet counts, the data frame should have 16 rows.


## Building the randomisation distribution

To perform the randomisation required for the hypothesis test, we need
to shuffle the fitted model residuals. Therefore, we fit the model and
compute the fitted values and residuals.

```{r eval=FALSE}
library(dae) # for yates.effects()

m = aov(count ~ when + company, data = d) # fit the Null model (no gamma)
f = predict(m)   # computed expected values
r = residuals(m) # compute residuals
```

We now need to shuffle the residuals, add them back to the fitted
values, then recompute the model with the interaction term included.

```{r eval=FALSE}
y = f + sample(r) # shuffle residuals and add to fitted
x = yates.effects(aov(y ~ when * company, data = d)) # compute the model coefficients using the new data y
interaction = x[3] # store the interaction coefficient
````

Write the code to generate 1000 interaction coefficients using
shuffled residuals. Store the 1000 interaction coefficients in the varaible `randDist`.
Examine the histogram of `randDist`.

## Computing the $p$ value

To finish the test, compute the interaction coefficient from the data:
```{r eval = FALSE}
z = yates.effects(aov(count ~ when * company, data = d)) # compute the model coefficients
gamma = x[3]
```

And compare it to `randDist` to compute the $p$ value.



## More data

Use your code to test the following data.

```{r echo=FALSE}
sim.two(140, 175, 75,75, 5)
sim.two(200,210, 150,145, 6)
```


## Using AOV

R has built in functions to analyse this kind of data. It is a special
case of a statistical technique called _analysis of variance_.

Using the data frame `d`, run the code:
```{r eval=FALSE, prompt=FALSE}
summary(aov(count ~ when*company, data = d))
```

This produces output, like the following,

```{r echo=FALSE}
before.control = c(43, 47, 58, 61)
after.control = c(81, 78, 90, 79)
before.impact = c(99, 116, 107, 93)
after.impact = c(129, 143, 119, 120)
data = sqrt(c(before.control, after.control, before.impact, after.impact))
n = length(before.control)
when = c(rep("before", n), rep("after", n), rep("before", n), rep("after", n))
company = c(rep("control", 2 * n), rep("impact", 2 * n))
summary(aov(data ~ when * company))
```

The result we are interested in, is the row corresponding to `when:company` in this table. 
It has the $F$-statistic and the $p$ value `Pr(>F)` for the interaction.
Compare this to the $p$ value computed using randomisation.


# Broken Stick models   (Extra material for those interested)

All of the above has ignored any _trend_ in the count data. Last week we looked at estimating trend, 
and this weeek we looked at detecting impact ignoring any trend. Can we detect an impact in data that has a trend?

In the simplest case think of a linear trend (straight line), on the square root scale. 
Suppose the impact changes the slope of the line. ie. after the event (advertisng etc.), 
the trend in tweets increases.

```{r lab12-sl, echo=FALSE}
day = 1:45
int = 20
mu = 10+ 0.04*day + 0.04*pmax(day-int,0)
plot(day, mu, ylab="trend", lwd=2, type="l")
abline(v=int, col="red", lwd=2)
text(int, par("usr")[4]-1, paste("Event at day", int), adj=0.5)
```

Tweet count data sampled from this might look like;

```{r lab12-tc, echo=FALSE}
set.seed(12245)
count = rpois(length(day), mu^2)
plot(day, count, pch=16)
```


And if we fit a linear trend (ignoring the event) we get (on a square root scale)

```{r lab12-lt, echo=FALSE}
plot(day, sqrt(count), pch=16)
abline((fit0 = lm(sqrt(count)~day)), lwd=2, col="blue")
```


Remember that the equation of line can be written

$$y = \alpha + \beta t$$

and $\beta$ is the slope. If we want to have a different slope after an event that occurs at time $t_0$ we can use,

$$y = \alpha + \beta t  + \gamma \max(t-t_0,0)$$

The last term here is zero for times $t$ less than $t_0$ and linear afterwards.

```{r lab12-zero, echo=FALSE}
plot(day, pmax(day-int,0), type="l", lwd=2, ylab=expression(paste("max(", t-t[0], ",0)")))
```

We can fit this in R using the following.

```{r lab12-sm, echo=c(1,5)}
fit = lm(sqrt(count)~day + I(pmax(day-20,0)))
plot(day, sqrt(count), pch=16)
abline(lm(sqrt(count)~day), lwd=2, col="blue")
lines(day, fitted(fit), lwd=2, col="red")
summary(fit)
```

Notice that the term for `I(pmax(day-20,0))` is significant ($p$-value less than 0.05)

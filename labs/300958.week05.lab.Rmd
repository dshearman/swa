% Twitter and Facebook APIs 
% 300958 Social Web Analysis 
% Week 5 Lab



# Installing Libraries

Installing libraries requires access to the Internet and hence R needs
to know about the Web proxy when within UWS.

Information on how to tell R about the proxy is here:

- http://support.rstudio.org/help/kb/faq/configuring-r-to-use-an-http-proxy

Information about the proxy setting is here:

- http://www.uws.edu.au/information_technology_services/its/servicedesk/proxy_settings


Once this has been done, you may install libraries using the install.packages command:
```{r eval=FALSE}
install.packages("twitteR")
```
If R will not speak to the proxy, you can download the packages and install them locally.
Get the package from here:

- http://cran.r-project.org/web/packages/available_packages_by_name.html

Install the package by telling R where you saved it:
```{r eval=FALSE}
install.packages("/tmp/bigdata_0.1.tgz", repos=NULL)
```

# Accessing Twitter

To access the Twitter API, a Twitter account is needed. If you do not have one, sign up here:

- https://twitter.com/

To access the Twitter API, we must register a program that is accessing the API. To register the program, visit this link:

- https://dev.twitter.com/apps

and log in. Once you have logged in, click "Create a new application"
and fill in the form. When the application is created, it will be
provided a consumer key and a consumer secret; these text sequences
are needed soon.

We can access the Twitter API in R by loading the twitteR and ROAuth libraries (and
installing them first if needed).

`r library("twitteR")`
```{r}
library("twitteR")
library("ROAuth")
```

We then need to set up the OAuth session details. Note: insert your
applications key and secret where needed.

```{.r}
cred <- OAuthFactory$new(consumerKey="insert here",
     consumerSecret="insert here",
     requestURL="https://api.twitter.com/oauth/request_token",
     accessURL="https://api.twitter.com/oauth/access_token",
     authURL="https://api.twitter.com/oauth/authorize")
cred$handshake()
```

Once the handshake is performed, R will ask you to visit a URL to get
an access code from Twitter. Get this code and provide it to R.
Once the handshake is done, we can save the credential information
and use it later, without having go directly to Twitter again.

```{.r}
save(cred, file="twitter.Rdata")
```

When we want to access Twitter in another session, we simply load and
register the credentials:
```{.r}
load("twitter.Rdata")
registerTwitterOAuth(cred)
```


```{.r}
x = searchTwitter('kevin bacon')

x = searchTwitter('social web analytics', n = 500, lang = "en")
z = twListToDF(x)



library("wordcloud")
library("tm")

cz = Corpus(VectorSource(z$text))
#cz = tm_map(cz, function(x) iconv(enc2utf8(x), sub = "byte"))
cz = tm_map(cz, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
cz <- tm_map(cz, removeNumbers)
cz <- tm_map(cz, removePunctuation)
cz <- tm_map(cz, stripWhitespace)
cz <- tm_map(cz, tolower)
cz <- tm_map(cz, removeWords, stopwords("english")) # this stopword file is at C:\Users\[username]\Documents\R\win-library\2.13\tm\stopwords 
cz <- tm_map(cz, removeWords, c("social", "web", "analytics"))
cz <- tm_map(cz, function(x) sub("http[^ ]*","", x))
cz <- tm_map(cz, stemDocument, language = "english")
wordcloud(cz)


library(tm) #load text mining library
setwd('F:/My Documents/My texts') #sets R's working directory to near where my files are
a  <-Corpus(DirSource("/My Documents/My texts"), readerControl = list(language="lat")) #specifies the exact folder where my text file(s) is for analysis with tm.
summary(a)  #check what went in
a <- tm_map(a, removeNumbers)
a <- tm_map(a, removePunctuation)
a <- tm_map(a , stripWhitespace)
a <- tm_map(a, tolower)
a <- tm_map(a, removeWords, stopwords("english")) # this stopword file is at C:\Users\[username]\Documents\R\win-library\2.13\tm\stopwords 
a <- tm_map(a, stemDocument, language = "english")
adtm <-DocumentTermMatrix(a) 
adtm <- removeSparseTerms(adtm, 0.75)


cz = Corpus(VectorSource(z$text))
tdm <- TermDocumentMatrix(cz, control = list(removePunctuation = TRUE, stopwords = TRUE, stem = TRUE))
tdm = weightTfIdf(tdm)
inspect(tdm)
tdm2 <- removeSparseTerms(tdm, sparse=0.95)
df <- as.data.frame(inspect(tdm2))
d = dist(df)
h = hclust(d)
rect.hclust(h, k=5, border="red")


cos.dissim <- function(x,y) {
	   x = as.numeric(x)
	   y = as.numeric(y)
	   return(1 - (x %*% y)/(sqrt(x %*% x)*sqrt(y %*% y)))
}

cos.sim <- function(x,y) {
	   x = as.numeric(x)
	   y = as.numeric(y)
	   return((x %*% y)/(sqrt(x %*% x)*sqrt(y %*% y)))
}

d = matrix(0, nrow(df), nrow(df))

for (a in 1:nrow(df)) {
    for (b in 1:nrow(df)) {
    	d[a,b] <- cos.dissim(df[a,],df[b,])
    }
}
rownames(d) <- rownames(df)

d = as.dist(d)
h = hclust(d)
plot(h)

```

http://files.meetup.com/1772780/Mining%20Twitter%20Using%20R.pdf

# Accessing Facebook




library("twitteR")
library("tm")
library("wordcloud")


# show the difference in weight and raw frequency
library("tm")
data(crude)
a <- tm_map(crude, removeNumbers)
a <- tm_map(a, removePunctuation)
a <- tm_map(a , stripWhitespace)
a <- tm_map(a, tolower)
a <- tm_map(a, removeWords, stopwords("english"))
a <- tm_map(a, stemDocument, language = "english")

A = TermDocumentMatrix(a)
B = TermDocumentMatrix(crude)
W = weightTfIdf(A)

D = as.data.frame(inspect(A))
E = as.data.frame(inspect(B))
F = as.data.frame(inspect(W))

t = table(apply(D, 1, sum))
s = table(apply(E, 1, sum))

# frequency
rownames(E)[order(E[,17], decreasing=TRUE)][1:10]
# weights
rownames(F)[order(F[,17], decreasing=TRUE)][1:10]

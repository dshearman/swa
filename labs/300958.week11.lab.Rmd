% Looking for Trends 
% 300958 Social Web Analysis 
% Week 11 Lab

```{r setup, include=FALSE}
opts_chunk$set(fig.path='figure/week11-lab-')
```

`r opts_chunk$set(prompt=TRUE, comment=NA)`

# Twitter Trends

Complete the following R-code to write a function to compute Twitter's
$\chi^2$ statistic for trends.

```{r eval=FALSE, prompt=FALSE}
TwitterChi = function(tweetsBefore, tweetsNow, N) {

  ## tweetsBefore is the number of tweets out of N on a topic of
  ## interest in a previous period tweetsNow is the number of tweets
  ## out of N on a topic of interest in the current period N is the
  ## number of tweets the samples
  
  
}
```

Test your code on the following 3 data sets.

```{r echo=FALSE, results="asis"}
suppressWarnings(require(xtable, quietly=TRUE))
set.seed(5272)
sim = function(p1,p2,N) {
  X = matrix(c(rbinom(2, N, c(p1,p2)),N), nrow=1)
  dimnames(X) = list("Number of Tweets", c("Before", "Now", "Sample Size"))
  print(xtable(X, digits=0), type="html")
  invisible(X)
}
x1 = sim(0.05,0.07, 1000)

x2 = sim(0.02,0.05, 10000)

x3 = sim(0.02,0.05, 20000)
```

## Test for independence between past and current tweets

We saw in the lecture that if there is a trend, then the tweet counts
of tweets containing the topic, will be dependent on time. We can
examine if there is a dependence by performing a $\chi^2$ test for
independence (just as we did in the Simple Exposure Analysis lab).

Using the data from the previous section, create the $2\times 2$ table
(contains topic/does not contain topic vs. before/now) and perform the
randomisation test for independence using your code from the Simple
Exposure Analysis lab.

## Using R's test for independence

Use the function `chisq.test` to compute the test $p$-value and
compare the results with your randomisation test.

## Which method is correct?

Since Twitter probably only use these $\chi^2$ statistics for ranking,
it probably doesn't matter which is used.

# Linear Regression for trends


## Tweet count data

Using the data below, fit the linear regression model and identify the
values of the intercept and the gradient. Also identify if the
gradient is significant (identify if the population gradient is not
likely to be zero).

```{r prompt=FALSE, results="hide"}
count = c(135, 145, 133, 102, 105, 108, 128, 144, 
149, 130, 107, 106, 83, 117, 123, 104, 116, 127, 75, 128, 136, 
145, 120, 127, 110, 109, 122, 136, 125, 143, 177, 142, 134, 153, 
173, 151, 174, 168, 158, 156, 128, 156, 140, 133, 132)
day = c(210, 
211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 
224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 
237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 
250, 251, 252, 253, 254)
```

You should use, $x=\text{day}$ and $y=\sqrt{\text{count}}$.

To fit and analyse the model, use the functions `lm` (linear model), and
`summary` (to examine the summary of the model).


# Trend, Seasonality, Irregular decomposition

In lectures, we looked at a simple method using moving averages, to
compute trend and seasonal components.  Here we will look at how to do
this in R. The data from lectures are the counts given above, but we
will work with square roots.

```{r}
Y = sqrt(count)
```

## Moving averages using a loop

The following is a moving average (as defined in lectures) function
using a `for` loop.  The function `moving.average` calls the function
`windowWeights` to obtain the set of weights used when combining the
windowed values. When the window length $m$ is odd, we want the
average of the window, this is the same as multiplying each value in
the window by $1/m$, then summing the values. When the window length
is even, we increase the window length by 1 to make it odd (so that it
have a centre), but we half the end elements of the window before
computing the average. So the window weights become 1/(2m) at the ends
and 1/m for the remaining positions.

```{r prompt=FALSE}
windowWeights = function(m) {
  if( m%%2 ) { 
     ## m is odd
     w = rep(1,m)/m
   } else { 
    ## m is even

    ## insert code here to compute w for an even sized window
  }
  return(w)
}

moving.average = function(x, m) {

  ## compute window weights
  w = windowWeights(m)

  j = floor(m/2)
  offsets = (-j):j
  n = length(x)
  res = rep(NA, n)
  ## slide window and apply window weights to obtain averages
  for(i in (j+1):(n-j)) {
    res[i] = sum(w*x[i+offsets])
  }
  return(res)
}
```

The above function `windowWeights` is missing a line of code. Write
the missing code, then use the functions to compute the trend
components of the data. Remember to square root first.

## Seasonal component

The following function computes a seasonal component by averaging over
all observations that are $m$ apart.

```{r prompt=FALSE}
seasonal = function(x ,m) {
  ## extend the data so it is a multiple of m long
  tmp = c(x, rep(NA, m - length(x)%%m))
  ## convert to a matrix
  mat = matrix(tmp, nrow=m)
  ## Calculate the row means to get the seasonal component (excluding missing entries)
  seas = rowMeans(mat, na.rm=TRUE)
  seas = seas - mean(seas)
  return(seas)
}
```

Make sure you understand this function and use it to compute the
seasonal component for the tweet count data. Remember to subtract the
trend first.

## R function

Of course, R has all this functionality built in, using the function
`decompose`. R expects the data to be a _time series_ which is a
special R data structure.

```{r lab11-yts, fig.width=7, fig.height=5, fig.cap=""}
Yts = ts(sqrt(count), freq=7)
plot(decompose(Yts), xlab="Week")
```

 

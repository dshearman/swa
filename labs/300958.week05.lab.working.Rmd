

ft = apply(tweet.matrix > 0, 2, sum)
N =  dim(tweet.matrix)[1]
wdt = log(tweet.matrix + 1) %*% diag(log(N/ft))



word.pos = order(apply(xdt,2,sum), decreasing=TRUE)[1:10] # position of top words
colnames(tweet.matrix)[word.pos]




```{.r}
library("wordcloud")
library("tm")

cz = Corpus(VectorSource(z$text))
#cz = tm_map(cz, function(x) iconv(enc2utf8(x), sub = "byte"))
cz = tm_map(cz, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
cz <- tm_map(cz, removeNumbers)
cz <- tm_map(cz, removePunctuation)
cz <- tm_map(cz, stripWhitespace)
cz <- tm_map(cz, tolower)
cz <- tm_map(cz, removeWords, stopwords("english")) # this stopword file is at C:\Users\[username]\Documents\R\win-library\2.13\tm\stopwords 
cz <- tm_map(cz, removeWords, c("social", "web", "analytics"))
cz <- tm_map(cz, function(x) sub("http[^ ]*","", x))
cz <- tm_map(cz, stemDocument, language = "english")
wordcloud(cz)


library(tm) #load text mining library
setwd('F:/My Documents/My texts') #sets R's working directory to near where my files are
a  <-Corpus(DirSource("/My Documents/My texts"), readerControl = list(language="lat")) #specifies the exact folder where my text file(s) is for analysis with tm.
summary(a)  #check what went in
a <- tm_map(a, removeNumbers)
a <- tm_map(a, removePunctuation)
a <- tm_map(a , stripWhitespace)
a <- tm_map(a, tolower)
a <- tm_map(a, removeWords, stopwords("english")) # this stopword file is at C:\Users\[username]\Documents\R\win-library\2.13\tm\stopwords 
a <- tm_map(a, stemDocument, language = "english")
adtm <-DocumentTermMatrix(a) 
adtm <- removeSparseTerms(adtm, 0.75)


cz = Corpus(VectorSource(z$text))
tdm <- TermDocumentMatrix(cz, control = list(removePunctuation = TRUE, stopwords = TRUE, stem = TRUE))
tdm = weightTfIdf(tdm)
inspect(tdm)
tdm2 <- removeSparseTerms(tdm, sparse=0.95)
df <- as.data.frame(inspect(tdm2))
d = dist(df)
h = hclust(d)
rect.hclust(h, k=5, border="red")


cos.dissim <- function(x,y) {
	   x = as.numeric(x)
	   y = as.numeric(y)
	   return(1 - (x %*% y)/(sqrt(x %*% x)*sqrt(y %*% y)))
}

cos.sim <- function(x,y) {
	   x = as.numeric(x)
	   y = as.numeric(y)
	   return((x %*% y)/(sqrt(x %*% x)*sqrt(y %*% y)))
}

d = matrix(0, nrow(df), nrow(df))

for (a in 1:nrow(df)) {
    for (b in 1:nrow(df)) {
    	d[a,b] <- cos.dissim(df[a,],df[b,])
    }
}
rownames(d) <- rownames(df)

d = as.dist(d)
h = hclust(d)
plot(h)

```

http://files.meetup.com/1772780/Mining%20Twitter%20Using%20R.pdf

# Accessing Facebook




library("twitteR")
library("tm")
library("wordcloud")


# show the difference in weight and raw frequency
library("tm")
data(crude)
a <- tm_map(crude, removeNumbers)
a <- tm_map(a, removePunctuation)
a <- tm_map(a , stripWhitespace)
a <- tm_map(a, tolower)
a <- tm_map(a, removeWords, stopwords("english"))
a <- tm_map(a, stemDocument, language = "english")

A = TermDocumentMatrix(a)
B = TermDocumentMatrix(crude)
W = weightTfIdf(A)

D = as.data.frame(inspect(A))
E = as.data.frame(inspect(B))
F = as.data.frame(inspect(W))

t = table(apply(D, 1, sum))
s = table(apply(E, 1, sum))

# frequency
rownames(E)[order(E[,17], decreasing=TRUE)][1:10]
# weights
rownames(F)[order(F[,17], decreasing=TRUE)][1:10]

% Visualisation of Social Web Data
% 300958 Social Web Analysis 
% Week 7 Lab

```{r setup, include=FALSE}
opts_chunk$set(fig.path='figure/week07-lab-')
```

`r opts_chunk$set(prompt=TRUE, comment=NA)`

# Required libraries

You will need the `twitteR`, `tm` and `wordcloud` packages.

# Political tweets

In the lecture, we showed how to download the tweets mentioning one of
three politicians, and then create a term document matrix containing
the frequency of each word in each document.

In this lab we will download the tweets published by the leaders of
the three political parties:

- <https://en.wikipedia.org/wiki/Australian_Labor_Party>
- <https://en.wikipedia.org/wiki/Liberal_Party_of_Australia>
- <https://en.wikipedia.org/wiki/Australian_Greens>

To do this,

1. visit the above wikipedia pages to locate the names,
2. then search for their twitter handles (beginning with @) using a Web browser.
3. Once you have found the three handles use `userTimeline(handle, n=100, lang = "en")` for each handle, storing the results in `tweets1`, `tweets2` and `tweets3`, to obtain their most recent 100 tweets.
4. Combine the set of tweets into the vector `tweets`
5. Then create the term document matrix for the set of tweets (as done in the week 5 lab).

In the lecture, we set `stopwords = stoplist` in the function
`TermDocumentMatrix`. We don't have a stoplist yet, so set `stopwords = TRUE`
to use the default stoplist.

If you have network problems, the tweet examples from lectures can be used:
[Tweets.RData](Tweets.RData). This can be read using `load("Tweets.RData")`
Doing so will provide you with `tweets1`, `tweets2`, and tweets3`.


# Draw a Wordle-esque word cloud

To draw a word cloud, we need a set of words and the weight of each word.

1. Create the word weight by summing the term frequencies (using either `rowSums` or `colSums`).
2. Extract the names of the words using the function `names` on the above weight vector.
3. Using the function `wordcloud` from the library `wordcloud` to draw the word cloud. Adjust the value of min.freq to make the word cloud visually better.

How does the word cloud look?

Create another word cloud, but use a TF-IDF weighted matrix. Does the
word cloud look better or worse?

Finally, if there are words that are not informative, create a
stoplist containing the words to be removed and the words from
`stopwords()`.

When you are happy with the results, sit back and take a minute to be
proud of your work, then move on to the next section.


# Principal Components Analysis

Using the term-document matrix from above, produce a visualisation plot
of the first two principal components. Remember that we are using a
TF-IDF weighted term frequencies.

First we generate a vector of colours to represent the three political leaders.

```{r eval=FALSE, tidy=FALSE}
colours = c(rep("red", length(tweets1)), 
           rep("blue", length(tweets2)), 
           rep("green", length(tweets3)))
```

Then compute the set of principal components and plot the first two.
```{r eval=FALSE}
pca <- prcomp(t(M))
plot(pca$x[,1], pca$x[,2], col=colours, pch=16)
```

Examine the PCA summary to identify the variance that the first few components represent.
Which of the two leaders tweet the most similar material?

Try and plot the first and third principal components. And the second and third.

Repeat using a `sqrt` transformation and without TF-IDF weighting. Is
there a difference in the variance that the first few components have
captured? Would you recommend using the TF-IDF or square root
transformation?  Compare your results to your computer lab neighbour.

# Multidimensional Scaling

We showed how to compute MDS projections in the lecture.  Compute and
plot the MDS 2D projection of the tweets using Euclidean distance, and
verify that it gives the same results as PCA.

Compute the MDS projection of the unweighted tweets (using term
frequency) using the Binary distance and Cosine distance. Then examine
the MDS projection with the same distances, but using TF-IDF
weighting. Did using TF-IDF weighting make a difference?



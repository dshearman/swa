% Text Mining 2: Clustering
% 300958 Social Web Analysis 
% Week 8 Lab

```{r setup, include=FALSE}
opts_chunk$set(fig.path='figure/week08-lab-')
```


# Preliminaries

Before we can begin the lab work, we must obtain a text data set and
convert it into a document term matrix.

## Requirements

For this lab, we will be clustering the text from a set of tweets.
Before we can begin, we must load the relevant libraries.
```{r eval=FALSE}
# Use to access Twitter and the twitteR class
library("twitteR")
# Use to convert tweets to a term frequency matrix
library("tm")
# Use to stem the tweet text
library("SnowballC")
```

## Prepare data

We will use the set of tweets obtained in the Visualisation lab. First
load the tweets: Download the tweet file
[Tweets.Rdata](http://staff.scm.uws.edu.au/~lapark/300958/labs/Tweets.RData)
to your **working directory**, and load the file:
```{r eval=FALSE}
 load("Tweets.RData")
```
Extract the tweets into data frames and combine the tweet text into a vector:
```{r eval=FALSE}
df1 = twListToDF(tweets1)
df2 = twListToDF(tweets2)
df3 = twListToDF(tweets3)
tweet.text = c(df1$text, df2$text, df3$text)
```
Now that we have the text in a vector, we can create a corpus:
```{r eval=FALSE}
tweet.corpus = Corpus(VectorSource(tweet.text))
```
then convert the characters to ASCII:
```{r eval=FALSE}
tweet.corpus = tm_map(tweet.corpus, function(x) iconv(x, to='ASCII'))
```
Finally, we will remove unwanted characters, remove stopwords and stem the words:
```{r eval=FALSE}
tweet.corpus = tm_map(tweet.corpus, removeNumbers)
tweet.corpus = tm_map(tweet.corpus, removePunctuation)
tweet.corpus = tm_map(tweet.corpus, stripWhitespace)
tweet.corpus = tm_map(tweet.corpus, tolower)
tweet.corpus = tm_map(tweet.corpus, removeWords, stopwords('english'))
tweet.corpus = tm_map(tweet.corpus, stemDocument)
tweet.corpus = tm_map(tweet.corpus, PlainTextDocument)
```
To perform clustering, we must obtain the document vectors, contained in the document term matrix.
So we extract the matrix and apply TFIDF weighting:
```{r eval=FALSE}
tweet.dtm = DocumentTermMatrix(tweet.corpus)
tweet.wdtm = weightTfIdf(tweet.dtm)
tweet.matrix = as.matrix(tweet.wdtm)
## remove empty tweets
empties = which(rowSums(abs(tweet.matrix)) == 0)
tweet.matrix = tweet.matrix[-empties,]
```
We now have the set of weighted document vectors as rows of the matrix `tweet.matrix`.

# K-means

In this section, we will examine the capability of k-means clustering
to produce meaningful clusters for our document term matrix.

R provides the function `kmeans` to perform k-means clustering. Read
the help page for `kmeans` to examine its arguments and what it
returns.

## Clustering Irises

To begin, we will cluster a small data set called `iris` that comes
packaged with R. To view the data:
```{r eval=FALSE}
print(iris)
```
We know this data is a sample of three types of irises, so we will use
k-means to obtain three clusters. The last column of the data is the
type of iris, so we will remove it so that k-means is not given the
answer to the clustering. The function `kmeans` take the data and the
number of clusters as its arguments.
```{r eval=FALSE}
K = kmeans(iris[,-5],3)
```
To visualise the clustering, we will use multidimensional scaling to
project the data into a 2d space.
```{r eval=FALSE}
iris.2d = cmdscale(dist(iris[,-5]))
```
We plot the data, using the computed clusters to colour the data, and
set the plot symbols using the real iris types.
```{r eval=FALSE}
plot(iris.2d, col = K$cluster, pch = as.numeric(iris[,5]))
legend("bottomright", levels(iris[,5]), pch = c(1,2,3))
```
Note that we also added a plot legend using the `legend` command.

**How** did k-means perform? Did it the iris types cluster well?


## Euclidean K-means

We can perform k-means clustering on our tweets. To perform the
clustering, we provide the data as a matrix and the number of clusters
we want.
```{r eval=FALSE}
K = kmeans(tweet.matrix, 10)
```
When the k-means process has finished, we can examine the cluster centres:
```{r eval=FALSE}
K$centers
```
and the cluster allocation of each tweet:
```{r eval=FALSE}
K$cluster
```
Since there are so many tweets, we can tabulate the cluster
allocation, to see how many tweets are associated to each cluster:
```{r eval=FALSE}
table(K$cluster)
```
For this case, it is likely that there is one cluster that contains
most of the tweets. This implies that we have a poor clustering of the
data.

Let's visualise the data (using Multidimensional scaling from a
previous lab) to examine our assumption. First, we will project the
data into a 2d space.
```{r eval=FALSE}
tweet.matrix.2d = cmdscale(dist(tweet.matrix))
```
Then plot it, and colour the points according to the given clusters.
```{r eval=FALSE}
plot(tweet.matrix.2d, col = K$cluster)
```
Are the clusters well separated?


## Number of clusters

We found in the last section that the clustering was poor, since most
of the tweets were allocated to the same cluster. In this section, we
will use the Elbow method to examine the number of clusters that are
suitable for our tweet data.

The Elbow method requires us to plot the within sum of squares (SSW)
against the number of clusters. Therefore, we must compute a
clustering for a set of cluster sizes, and obtain the SSW value.

We can compute a clustering with one cluster:
```{r eval=FALSE}
number.of.clusters = 1
K = kmeans(tweet.matrix, number.of.clusters)
```
and obtain the SSW value:
```{r eval=FALSE}
K$tot.withinss
```
Note that when we compute one cluster, the within sum of squares (SSW)
is equal to the total sum of squares (SST).
```{r eval=FALSE}
K$totss
```
We want to compute the SSW for cluster sizes from 1 to 15. First we will allocate
a vector variable to store the value of SSW:
```{r eval=FALSE}
SSW = rep(0,15)
```
**Write** a for loop that performs k-means for 1 to 15 clusters and stores the 
SSW value for each clustering in the variable `SSW`.

Once we have the values of SSW for 1 to 15 clusters, we can view the elbow plot:
```{r eval=FALSE}
plot(1:15,SSW, type="b")
```
From the plot, we will see that there is an elbow at 9 clusters, but the SSW drops afterwards, meaning that there may be a better clustering at a higher number of clusters. This is a large number of clusters, meaning that we are not clustering the data correctly.

A likely cause of the poor clustering is that k-means uses Euclidean distance,
which is not appropriate for document vectors. We need to use k-means with the
cosine similarity function, so we need to comine Multi-dimensional scaling with K-means.


## K-means with cosine distance

The k-means algorithm only clusters using Euclidean distance,
therefore to use Cosine distance, we must convert the cosine distances
to Euclidean distance. To do this, we use Multi-dimensional scaling (MDS).


To perform k-means using Cosine distance, we first create the distance matrix,
containing Cosine distances.

```{r eval=FALSE}
## first normalise all tweets to unit length (divide by their norm)
norm.tweet.matrix = diag(1/sqrt(rowSums(tweet.matrix^2))) %*% tweet.matrix
## then create the distance matrix
D = dist(norm.tweet.matrix, method = "euclidean")^2/2
## perform MDS using 300 dimensions
mds.tweet.matrix <- cmdscale(D, k=300)
```
We now have the set of tweets projected into a 300 dimsional Euclidean space.

**Use** the elbow method to identify an appropriate number of
  clusters, then compute the clusters and store them in `K`.


To finish this section, we will visualise the data using multidimensional scaling.
We first project the tweets into a 2D space for visualisation.
```{r eval=FALSE}
## perform MDS using 2 dimensions
mds2.tweet.matrix <- cmdscale(D, k=2)
```
Then we can colour the points using their cluster number.
```{r eval=FALSE}
plot(mds2.tweet.matrix, col = K$cluster)
```
**Are** the clusters visible after performing the multidimesional scaling?

## Examining the cluster contents.

To begin this section, we will run k-means on our data, using three clusters.
```{r eval=FALSE}
K = kmeans(mds.tweet.matrix, 3, nstart = 20)
```
We can now examine the top ten words associated to each cluster by
selecting the tweets associated to the cluster number, and ordering the words by the
mean TF-IDF weight in the cluster. For the first cluster, we use:
```{r eval=FALSE}
cluster.number = 1
## find position of tweets in cluster
clusterTweetsId = which(K$cluster == cluster.number)
## extract tweets vectors for cluster
clusterTweets = tweet.matrix[clusterTweetsId,]
## combine the tweets into a mean tweet
clusterTermWeight = colMeans(clusterTweets)
## show the top 10 weighted words
sort(clusterTermWeight, decreasing = TRUE)[1:10]
```
**Examine** the top ten words for the three clusters. Can you
  identify any themes associated to the clusters?

Looking at the top ten words gives us some information about the
cluster, but it is only a small fraction of the cluster information.

**Generate** a word cloud (as done in the Visualisation lab) to
  visualise the words in a cluster. Was this more helpful in
  identifying the cluster topics?

# Hierarchical Clustering

Hierarchical clustering us a useful tool to allow us to visualise how
clusters are formed in our data. But, with a large number of data
points, the hierarchy becomes crowed and difficult to visualise.

## Clustering Cars

For this first section, we will use a data set that comes packaged
with R, called `mtcars`. Read the help file for `mtcars` to get an
understanding of how the data was created.

To perform hierarchical clustering, we must first obtain the distance matrix:
```{r eval=FALSE}
D = dist(mtcars)
```
Then we provide the distance matrix to the hierarchical clustering function:
```{r eval=FALSE}
h = hclust(D)
```
To view the clustering, plot the output variable.
```{r eval=FALSE}
plot(h)
```
The default method is *complete linkage* clustering. To obtain *single
linkage* clustering, we must provide an additional argument to
`hclust`.
```{r eval=FALSE}
h = hclust(D, method="single")
plot(h)
```
**Look** at the dendrogram produced from each of the two clusterings
  and identify if they make sense.

## Clustering words

The twitter data we have has many words. If we examine the dimensions
of the matrix, the first value is the number of rows (documents), the
second value is the number of columns (words).
```{r eval=FALSE}
dim(twitter.matrix)
```

For this section, we will select the set of words that appear most
often and examine how they are clustered.

We will choose the set of words that appear in at least 50 tweets and
store their index in the variable `frequent.words`.
```{r eval=FALSE}
frequent.words = which(apply(tweet.matrix > 0,2,sum) > 50)
```
We now extract only those columns from the tweet matrix.
```{r eval=FALSE}
term.matrix = tweet.matrix[,frequent.words]
```

Since we are now clustering the terms and not the tweets, we must normalise the columns (not rows) when performing Cosine distance calculations.
```{r eval=FALSE}
norm.term.matrix = term.matrix %*% diag(1/sqrt(colSums(term.matrix^2)))
## preserve column names (terms associated to each column)
colnames(norm.term.matrix) = colnames(term.matrix)
```

**Compute** the distance matrix containing the Cosine distance between
  all chosen terms, then compute and plot the hierarchical
  clustering. Does changing from single linkage clustering to complete
  linkage clustering make a difference to the clustering results?



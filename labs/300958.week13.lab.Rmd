% Text Mining 3: Sentiment Analysis
% 300958 Social Web Analysis 
% Week 13 Lab


```{r setup, include=FALSE}
opts_chunk$set(fig.path='figure/week13-lab-')
opts_chunk$set(dev = 'png', fig.cap="", eval=FALSE)
```


# Preliminaries

To perform sentiment analysis of tweets, we need a set of tweets,
manually labelled with their sentiment.

## Load the data

The variable `tweet.sentiment` is available in the file
[tweet_sentiment.Rdata](http://staff.scm.uws.edu.au/~lapark/300958/labs/tweet_sentiment.Rdata). Download
the file to the working directory and load the file using:
```{r eval=FALSE}
load("tweet_sentiment.Rdata")
```

**NOTE**: These tweets have been posted by real twitter users and so
  many contain offensive or inappropriate content. If you feel that
  this content may upset you, you may use an alternative tweet set
  (e.g. twitter data used in previous labs).



Examine the contents of `tweet.sentiment` using the functions:

- `names` (to identify the table names)
- `dim` (to see the size of the table)

We find that the variable is a data.frame containing tweet text and
sentiment. To examine the levels of sentiment, use the function
`table` on `tweet.sentiment$sentiment`.

## Extract the wanted tweets

We find that there are more than two sentiment classes, but we want
only positive and negative. We can extract the positive and negative
sentiment tweets using the function `subset`:
```{r}
tweet.sentiment = subset(tweet.sentiment, sentiment == "positive" | sentiment == "negative")
```

## Arrange the data

The tweets are given as text. Use functions in the library `tm` to
convert the set of tweets into a document-term matrix called
`tweet.matrix`. The details of this process are found in the
[Text Mining section of lab
5](http://staff.scm.uws.edu.au/~lapark/300958/labs/300958.week05.lab.html#text-mining).
Note: we want a document-term matrix containing term frequencies, not
weighted term frequencies. Do not use TF-IDF weights.


## Training and Validation split

To evaluate each of the classification methods, we must split the data
into a training and validation sets.

Let's randomly choose 50% of the tweets for training:
```{r}
tweet.count = nrow(tweet.matrix)
train.positions = sample(tweet.count,round(0.5*tweet.count))
A.train = tweet.matrix[train.positions,]
C.train = tweet.sentiment$sentiment[train.positions]
```
and the remaining 50% for validation.
```{r}
A.validate = tweet.matrix[-train.positions,]
C.validate = tweet.sentiment$sentiment[-train.positions]
```

We now have a set of vectors and classes for training and validation
and can proceed to the classification task.

# Naive Bayes

The Naive Bayes classifier treats all variables as independent. The
variables of a tweet are the frequency of each word; if there are $m$
unique words, we have $m$ variables.

We showed in the lecture that a tweet can be classified using:
$$
\begin{align}
	L &= \log{\left( \frac{P(\text{S = Pos}\vert \text{tweet})}{P(\text{S = Neg}\vert \text{tweet})}\right )} \\
	&= \log{\left( \frac{P(\text{tweet}\vert \text{S = Pos})P(\text{S = Pos})/P(\text{tweet})}{P(\text{tweet}\vert \text{S = Neg})P(\text{S = Neg})/P(\text{tweet})}\right )}
\end{align}
$$
where the probability of the tweet is computed as:
$$
P(\text{tweet}|\text{S = Pos}) = {\prod_{\text{term} \in \text{tweet}}  P(\text{term}\vert \text{S = Pos})} \\
P(\text{tweet}|\text{S = Neg}) = {\prod_{\text{term} \in \text{tweet}}  P(\text{term}\vert \text{S = Neg})} \\
$$

The first term contains $P(\text{S = Pos})$, meaning the probability of a tweet
having positive sentiment, and $P(\text{S = Neg})$, meaning the probability of
a tweet having negative sentiment. $P(\text{S = Pos})$ is computed as:
```{r}
pS.Pos = mean(C.train == "positive")
```
**Compute** $P(\text{S = Neg})$ and store the value in the variable `pS.Neg`. 

The probability $P(\text{term}\vert \text{S = Pos})$ is the
probability of putting our hand in the bag of tweets with positive
sentiment and pulling one out that contains the given term.
Therefore $P(\text{term}\vert \text{S = Pos})$ is given as
$$
	P(\text{term}\vert \text{S = Pos}) = \frac{\text{Number of tweets with positive sentiment containing term}}{\text{Number of tweets with positive sentiment}}
$$
The tweet count for positive tweets is given as:
```{r}
total.positive.tweets = sum(C.train == "positive")
```
**Compute** the number of negative tweets and store in `total.negative.tweets`.

Computing the number of tweets containing a given term has been seen before when we compute the IDF in TF-IDF weighting. In this case we must limit the tweet set to only the positive tweets.
```{r}
term.positive.tweets = colSums(A.train[C.train == "positive",] > 0)
```

**Use** the above information to compute $P(\text{term}\vert \text{S =
Pos})$ and $P(\text{term}\vert \text{S = Neg})$ for all terms, and
store the result in the variables `pterm.S.Pos` and `pterm.S.Neg`.

Before we proceed, we must make sure that there are no zero
probabilities in $P(\text{term}\vert \text{S = Pos})$ or
$P(\text{term}\vert \text{S = Neg})$.  If there is a word that does
not appear in the positive or negative sentiment set, then we find
that tweets containing them get a sentiment score of either $\infty$
or $-\infty$.

To fix this problem, we can use the [Rule of
Succession](http://en.wikipedia.org/wiki/Rule_of_Succession).  This
requires us to add 1 to all values in `term.positive.tweets` and
`term.negative.tweets` and then recompute the sentiment weights. Do
this now if there are zero probabilities.


Let's now compute the sentiment of the first validation tweet. We need to compute:

- $P(\text{tweet}|\text{S = Pos})$
- $P(\text{tweet}|\text{S = Neg})$
- $P(\text{tweet})$

We need to multiply the probability of each term in the tweets. To do
this, we can take each probability to the power of their frequency in
the validation tweet and multiply the result. Remember that $x^0 = 1$,
so if the word does not appear, we take the probability to the power
of 0, making the result 1, meaning it has no effect.

So, $P(\text{tweet}|\text{S = Pos})$, where tweet is the first validation tweet, is computed as:
```{r}
ptweet.S.Pos = prod(pterm.S.Pos^A.validate[1,])
```
***Compute*** $P(\text{tweet}|\text{S = Neg})$ and store the result in `ptweet.S.Neg`.

***Use*** the above variables to compute $P(\text{tweet})$ using the equation shown in the lecture.

***Combine***
$P(\text{tweet}|\text{S = Pos})$,
$P(\text{tweet}|\text{S = Neg})$,
$P(\text{tweet})$,
$P(\text{S = Pos})$, and 
$P(\text{S = Neg})$ to compute the sentiment score for the tweet and predict the sentiment class based on the score.

Now we will take what we have done and convert it into a function so
that we can use it on many validation tweets.

***Complete*** the below function to compute the sentiment for a given tweet vector:
```{r}
tweetSentiment = function(tweetVector, pterm.S.Pos, pterm.S.Neg, pS.Pos, pS.Neg) {

  ptweet.S.Pos = prod(pterm.S.Pos^tweetVector)
  ptweet.S.Neg = prod(pterm.S.Neg^tweetVector)

  ## Now we compute the probability of the tweet (shown in the lecture)

  ## compute the sentiment score

  return(score)
}
```

If the function is correct, this code should give the same sentiment
score as before:
```{r}
tweetSentiment(A.validate[1,], pterm.S.Pos, pterm.S.Neg, pS.Pos, pS.Neg)
```

Now we can classify the sentiment of all validation tweets, using a loop:
```{r}
validCount = nrow(A.validate)
scores = rep(0,validCount)
for (a in 1:validCount) {
    scores[a] = tweetSentiment(A.validate[a,], pterm.S.Pos, pterm.S.Neg, pS.Pos, pS.Neg)
}
```

And then convert the scores into predictions:
```{r}
predictedSentiment = scores
predictedSentiment[predictedSentiment > 0] = "Positive"
predictedSentiment[predictedSentiment < 0] = "Negative"
predictedSentiment[predictedSentiment == 0] = "Neutral"
```

We can now compare the computed sentiment to the actual sentiment using
a table.
```{r}
table(C.validate, predictedSentiment)
```

We can see there is some error (negative tweets classified as positive
and positive classified as negative). But the majority of the tweets
were classified correctly.




# k Nearest Neighbours

k Nearest Neighbours (kNN) computes the class of a tweet as the mode of the
$k$ nearest tweets. Therefore we have to define a distance metric over
the tweets. For this lab we will use the Binary metric.

kNN requires no training, therefore we can just go straight to the
classification stage.

We want to find the distance between a validation tweet and all of the training tweets. First we define the metric:
```{r}
binary.metric <- function(x,y) {
  return(mean(xor(x,y)))
}
```

Now we need to compute the distance between the tweet validation
vector and all of the training vectors.

**Complete** the function below:
```{r}
tweet.distance <- function(tweet, train.tweets) {
  # tweet is a row vector from A.validate,
  # train.tweets is A.train

  # return a vector of distances between tweet and all vectors in 
  # train.tweets measured using the function binary.metric
}
```
Once we have the distances, we can compute the kNN class:
```{r}
kNN.classify <- function(tweet, train.tweets, train.classes, k) {
  # identify the k closest tweets
  d = tweet.distance(tweet, train.tweets)
  closest.k.tweet.positions = order(d)[1:k]
  
  # select classes of closest tweets
  close.classes = train.classes[closest.k.tweet.positions]
  
  # return the most frequently appearing class (the mode)
  t = table(close.classes)
  return(names(which.max(t)))
}
```

The function `kNN.classify` computes the class of one tweet. **Write**
a function to compute the class of all tweets in the matrix
`A.validate`. Note that this classification of all validation tweets
will take a few minutes to run.


**Examine** the classification for different values of $k$. Does $k$ vary the results?

# Evaluation


In this section, we will examine how accurate our classifiers are. We
will examine the Sensitivity and Specificity.

Sensitivity is the proportion of positive class objects that were classified as positive.
We can compute this as:
```{r}
sensitivity <- function(predicted.class, actual.class) {
  positive.positions = which(actual.class == "positive")]
  return(mean(predicted.class[positive.positions] == "positive"))
}
```
Specificity is the proportion of negative class objects that were classified as negative.

**Write** the function `specificity` to compute the specificity of the prediction.

Once these functions have been written, **examine** the sensitivity and
specificity of the Naive Bayes and kNN results. Does one provide
better results that the other?  Is there a specific value of $k$ that
provides better classification?

**Plot** an ROC plot (plot 1 - specificity vs sensitivity, with plot
parameters xlim=c(0,1) and ylim=c(0,1)). Which classification
method is closer to the top left corner?

Remember that we randomly split the data into training and validation
sets. Compare these ROC plots with your friends to see if their
results from their random split is much different to ours.



% Text Mining 3: Sentiment Analysis
% Laurence A. F. Park
% 20th of September 2013

```{r setup, include=FALSE}
opts_chunk$set(fig.path='figure/sentiment-classification-')
opts_chunk$set(dev = 'pdf', fig.cap="")
```


### Motivation

Our company has released a product, we have found that it has made an
impact on the number of tweets about our company.


Is the increase in tweets due to the public's joy of our product, or
are tweeting on how annoyed they are of our new product?


\pause

We need to examine the sentiment of the tweets to determine this.


# Sentiment Analysis

### Sentiment

Our sentiment towards something is our opinion of it. In its simplest
form, sentiment can be positive or negative.

Tweets with positive sentiment:

- Team Austria Wins the 2013 Solar Decathlon With Their Net-Zero LISI House
- Bonus link, too cool to pass up ... two delicious fruit-named computers in one cool package.
- Who's excited for this Thursday's #TableTop?

Tweets with negative sentiment:

- Gravity: Can Film Ever Get the Science Right?
- Nobel Winners Illustrate Israel's "Brain Drain"
- Why Small-Scale Biomass Energy Projects Aren't a Solution To Climate Change


### Sentiment Analysis

Sentiment analysis is the process of labelling a piece of text with
its sentiment level. 

- Simple sentiment analysis classifies the polarity of a piece of text
  (positive or negative).

- Advanced sentiment analysis examines the emotion behind the text
  (e.g. excited, annoyed, surprised).

\pause

We can label the tweets by reading each marking it with a label, but
when we have thousands of tweets, and we want to do sentiment analysis
regularly, this process is too time consuming.

\pause

We will examine how to perform sentiment analysis using \alert{machine
learning}. More information on sentiment analysis:
<http://en.wikipedia.org/wiki/Sentiment_analysis>


### Machine Learning methods

Machine Learning is the process of providing a machine (computer) with
sample data, allowing it to learn from the data.

In this unit we have seen:

- Regression: learn how to predict a real value $y$ from a sample set of $y$ and $x$.
- Clustering: learn how to organise data from a sample $x$.

In this lecture we will introduce:

- Classification: learn how to predict a class $c$, from a sample set $c$ and $x$.

We will focus on binary classification, meaning there are two possible
classes.  These values are usually $\{True, False\}$, $\{\text{Yes},
\text{No}\}$, $\{0,1\}$. For sentiment analysis, they will be
$\{\text{Positive},\text{Negative}\}$.


### Classification

Classification is the task of assigning a class (category) to an
object based on its attributes (variables, features).

Classification is a similar task to regression, but regression
predicts a real value for each object.

Classification problems:

- Classify if this email is spam, given the words in the email.
- Classify if a patient has cancer, given the patients attributes.
- Classify if it will rain today, given the humidity of the past few days.
- Classify if there is a tank in this satellite image, given the pixel intensities and positions.
- Classify if a person is suitable for a job, given the contents of their Facebook page.

### Classification or Regression?

#### Problem

Which of the following prediction problems are classification problems and which are regression?

- Predict the number of days before it rains.
- Predict the colour of the rain.
- Predict the team to win the grand final.
- Predict the number of points scored in the grand final.
- Predict the words in the next tweet.
- Predict the increase in tweets per day.



### Classification is Supervised Learning

To classify the class of a data object $x$, we need to compute a
function $c = f(x)$, where $c$ is the class of $x$.

We compute the function $f(\cdot)$ using examples of existing data labelled with
its class. Computing this model using examples is called \alert{training}.

Once we construct the function, we are able to apply the function to
new data to determine its class.



### Classification Example

#### Problem: Bran for Breakfast

We surveyed six people and obtained the following information about
their age, gender, hair colour and if they have bran for breakfast.

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\rowcolor{blue!20}
                          & Age & Gender & Hair Colour & Bran  \\\hline
Person 1 & 7   & Male   & Brown       & No  \\\hline
Person 2 & 5   & Female & Black       & No  \\\hline
Person 3 & 12  & Male   & Black       & No  \\\hline
Person 4 & 32  & Female & Brown       & Yes \\\hline
Person 5 & 45  & Female & Blonde      & Yes \\\hline
Person 6 & 28  & Male   & Brown       & Yes \\\hline
\end{tabular}
\end{center}

Using this data, can we predict if a newly surveyed person has bran
for breakfast based on their age, gender and hair colour?

### Another Classification Example

#### Problem: Bran for Dinner

We surveyed six people and obtained the following information about
their age, gender, hair colour and if they have bran for dinner.

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\rowcolor{blue!20}
                          & Age & Gender & Hair Colour & Bran  \\\hline
Person 1 & 7   & Male   & Brown       & No  \\\hline
Person 2 & 5   & Female & Black       & No  \\\hline
Person 3 & 12  & Male   & Black       & No  \\\hline
Person 4 & 32  & Female & Brown       & No \\\hline
Person 5 & 45  & Female & Blonde      & Yes \\\hline
Person 6 & 28  & Male   & Brown       & No \\\hline
\end{tabular}
\end{center}

Using this data, can we predict if a newly surveyed person has bran
for dinner based on their age, gender and hair colour?


### Gathering Data for Classification Training

In the previous example, we could have used the classification functions:

- age > 32
- hair colour == blonde
- (age > 32) & (hair colour == blonde)

The model was ambiguous because we had only 1 sample for the ``Bran
for Dinner == Yes'' class.

To obtain a useful model, we generally require:

- a large sample size of both classes (the more the better)
- balanced sample (similar number of samples for each of the positive
  and negative classes)


### Classification of Sentiment


Given one thousand (or one million) tweets, it is difficult for us to
examine the tweet contents and determine rules that determine the
sentiment.

Therefore to perform sentiment analysis, we will use machine learning
to learn from a training set of examples, so we can then automatically
classify new pieces of text.

Machine learning methods require a set of objects, each with a set of
measured features and a class.  We want to examine the features and
make a prediction of the class.  When using tweets, the features are
the word frequencies (as we have used before) and the class is the
positive or negative sentiment.

### Common Classification Methods

There are different methods of classification that have different
levels of accuracy and varying computation times. The choice of
classification method depends on the classification problem.

Commonly used classification methods include:

- Logistic Regression
- Support Vector Machines
- Neural Networks
- Decision Trees
- $K$ nearest neighbours
- Naive Bayes


\pause

We will be examining $K$ nearest neighbours and Naive Bayes.

## K Nearest Neighbours


### $K$ Nearest Neighbours

The $K$ Nearest Neighbours classifier is one of the simplest
classifiers. It computes the class of a tweet as the \alert{mode} of
the classes of neighbouring tweets from the training set.  So if
the majority of the new tweet's neighbours have positive sentiment,
then the new tweet is classified as having positive sentiment.

Which tweets are considered neighbours? The $k$ closest tweets.
You choose the number $k$.

\pause

Since we are measuring distances, we must:

- choose an appropriate form for each tweet (we represent the tweet as
  a document vector containing weighted term frequencies).

- make sure we choose an appropriate distance metric (usually Cosine distance for tweets).


### $K$ Nearest Neighbours: Iris example

```{r, echo=FALSE, fig.width=4.4, fig.height=2.8}
I = cmdscale(dist(iris[,c(1:4)]))
C = c(rep(1,50),rep(2,50),rep(3,50)) + 1
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(I, col=C, xlab="", ylab = "")
```

### $K$ Nearest Neighbours: Iris example

```{r, echo=FALSE, fig.width=4.4, fig.height=2.8}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
x = c(-1,1)
k = 50
disty = function(x, y) { return(sqrt(sum((x - y)^2))) }
d = apply(I,1,disty, y = x)
topk = order(d, decreasing=FALSE)[1:k]
plot(I, col=C, xlab="", ylab = "")
points(x[1],x[2], col=1)
```

### $K$ Nearest Neighbours: Iris example

```{r, echo=FALSE, fig.width=4.4, fig.height=2.8}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
x = c(-1,1)
k = 50
disty = function(x, y) { return(sqrt(sum((x - y)^2))) }
d = apply(I,1,disty, y = x)
topk = order(d, decreasing=FALSE)[1:k]
plot(I, col=C, xlab="", ylab = "")
points(I[topk,1],I[topk,2], col=C[topk], pch=3)
points(x[1],x[2], col=1)
```

### $K$ Nearest Neighbours: Iris example

```{r, echo=FALSE, fig.width=4.4, fig.height=2.8}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
x = c(-1,1)
k = 50
m = as.numeric(names(which.max(table(C[topk]))))
disty = function(x, y) { return(sqrt(sum((x - y)^2))) }
d = apply(I,1,disty, y = x)
topk = order(d, decreasing=FALSE)[1:k]
plot(I, col=C, xlab="", ylab = "")
points(I[topk,1],I[topk,2], col=C[topk], pch=3)
points(x[1],x[2], col=m)
```

### $K$ Nearest Neighbours: Iris example $k = 1$

```{r, echo=FALSE, fig.width=4.4, fig.height=2.8}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)

k = 1
knn = function(x, I, C, k) {
    d = apply(I,1,disty, y = x)
    topk = order(d, decreasing=FALSE)[1:k]
    return(as.numeric(names(which.max(table(C[topk])))))
}
x1 = rep(seq(from=-3,by=0.1,to=4),21)
x2 = rep(seq(from=-1,by=0.1,to=1),each = 71)
X = cbind(x1,x2)
Xc = apply(X,1,knn, I, C, k)
plot(I, col=C, xlab="", ylab = "", pch=".")
points(x1,x2, col=Xc)
```

### $K$ Nearest Neighbours: Iris example $k = 10$

```{r, echo=FALSE, fig.width=4.4, fig.height=2.8}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)

k = 10
x1 = rep(seq(from=-3,by=0.1,to=4),21)
x2 = rep(seq(from=-1,by=0.1,to=1),each = 71)
X = cbind(x1,x2)
Xc = apply(X,1,knn, I, C, k)
plot(I, col=C, xlab="", ylab = "", pch=".")
points(x1,x2, col=Xc)
```


### $K$ Nearest Neighbours features

$K$ nearest neighbours is considered one of the extreme classifiers,
since its prediction depends on all of the training set, not just
statistics from the training set.

- It requires no training, but does require a training data set.
- All of the computation is performed when predicting the class of a
  point.
- We must store and access all of the training points in order to
  perform prediction.

### Classifying tweets with $k$ nearest neighbour

Preprocessing:

1. Obtain a set of tweets that have been labelled as having positive and negative sentiment.
2. Represent each tweet as a document vector.
3. Choose a distance function (usually Cosine distance).

\pause

To classify a new tweet:

1. Compute the distance between the new tweet and all of the training tweets.
2. Locate the $k$ training tweets with the smallest distances (closest).
3. The new tweet is classified with the mode of the $k$ closest tweets.

### kNN Problem

#### Problem

The distance of a new tweet to a set of classified tweets is shown below:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\rowcolor{blue!20}
Tweet ID & Sentiment & Distance to new tweet \\
\hline
42342 & Positive & 5 \\
23423 & Positive & 6 \\
85867 & Positive & 4 \\
12315 & Positive & 5 \\
\hline
23232 & Negative & 10 \\
46564 & Negative & 2 \\
26454 & Negative & 7 \\
66755 & Negative & 3 \\
\hline
\end{tabular}
\end{center}
What is the class of the new tweet using kNN with $k = 1$. Classify
using $k = 3$ and $5$ as well.



## Naive Bayes Classifier


### Using probability

Rather than computing distances between tweets, we can ask:

- What's the probability that this tweet has \alert{positive
  sentiment}, given the words it contains?, or

- What's the probability that this tweet has \alert{negative
  sentiment}, given the words it contains?


If the probability of being positive is greater than being negative,
then we classify the tweet as being positive, otherwise, we classify
it as being negative.

\pause

How do we compute these probabilities from the training set of tweets?

\pause

We use Bayes' theorem!


### Defining the problem

Using Bayes' theorem:

\begin{align*}
	P(\text{sentiment} = \text{Pos}\vert \text{tweet}) = \frac{P(\text{tweet}\vert \text{sentiment} = \text{Pos})P(\text{sentiment} = \text{Pos})}{P(\text{tweet})}
\end{align*}

Description:

- $P(\text{tweet}\vert \text{sentiment} = \text{Pos})$: If we were to
  randomly sample from all tweets with positive sentiment, what is the
  probability of obtaining this tweet?

- $P(\text{sentiment} = \text{Pos})$: If we were to sample from all
  tweets, what is the probability that the obtained tweet has positive
  sentiment?

- $P(\text{tweet})$: If we were to randomly sample from all tweets,
  what is the probability of obtaining this tweet?

\pause

How do we compute these probabilities from our training set of labelled tweets?

### Sentiment probability

Let's start with the easy one:

$P(\text{sentiment} = \text{Pos})$ is the probability of sampling a
tweet with positive sentiment. Using our training set, we can
estimate this as:

$$
P(\text{sentiment} = \text{Pos}) = \frac{\text{Number of positive tweets}}{\text{Number of tweets}}
$$

The same goes for the negative case:

$$
P(\text{sentiment} = \text{Neg}) = \frac{\text{Number of negative tweets}}{\text{Number of tweets}}
$$

### Tweet probability

$P(\text{tweet})$ is the probability of a given tweet, meaning when we
get a new tweet to classify we compute:
$$
P(\text{tweet}) = \frac{\text{Number of training tweets the same as the new tweet}}{\text{Number of tweets}}
$$

\pause
and also
$$
P(\text{tweet}\vert \text{sentiment} = \text{Pos}) = \frac{
\begin{array}{c}
\text{Number of \alert{positive} training tweets} \\ \text{the same as the new tweet}
\end{array}
}{\text{Number of \alert{positive}  tweets}}
$$

\pause

Due to the variation in English (or any language), the number of
training tweets the same as the new tweet is likely to be zero, so the
probability will be zero, meaning we cannot perform the
classification of new tweets. But we know the probability is not zero
(since the tweet has appeared).

\pause

How can we compute the probability of obtaining a given tweet?

### The Naive part of the Naive Bayes Classifier

The Naive Bayes classifier assumes that all terms in a tweet are
independent, so the probability of a tweet is the product of the
probability of the terms is contains.

$$
P(\text{tweet}) = {\prod_{\text{term}\in\text{tweet}} P(\text{term})}
$$

$$
P(\text{tweet}\vert \text{sentiment} = \text{Pos}) = {\prod_{\text{term}\in\text{tweet}} P(\text{term}\vert \text{sentiment} = \text{Pos})}
$$

\pause

The probability of sampling a given term $P(\text{term})$ is:
$$
P(\text{term}) = \frac{\text{Number of tweets containing the term}}{\text{Number of tweets}}
$$

$$
P(\text{term}\vert \text{sentiment} = \text{Pos}) = \frac{\text{Number of positive tweets containing the term}}{\text{Number of positive tweets}}
$$



### Working from term probabilities

Rather than examining all of the training tweets when classifying,
it is common to store only the values of:

- $P(\text{term}\vert \text{sentiment} = \text{Pos})$ and
- $P(\text{term}\vert \text{sentiment} = \text{Neg})$ for each term, and
- $P(\text{sentiment} = \text{Pos})$.


Using these, we can compute $P(\text{sentiment} = \text{Neg})$ as:
$$P(\text{sentiment} = \text{Neg}) = 1 - P(\text{sentiment} = \text{Pos})
$$

and we can compute $P(\text{term})$ as:
\begin{multline*}
P(\text{term}) = 
P(\text{term}\vert \text{sentiment} = \text{Pos})P(\text{sentiment} = \text{Pos}) \\
+ P(\text{term}\vert \text{sentiment} = \text{Neg})P(\text{sentiment} = \text{Neg})
\end{multline*}


### Gathering the probabilities

#### Example

Given the set of positive sentiment tweets:

- Having a fun time.
- Lost track of time, excited half of the day.
- Cats are enjoying themselves.

and the set of negative sentiment tweets:

- Lost the game. Sad day for cats.
- We lost at about half time.
- The half time whistle scared my cats.

Compute the probability of the terms "half", "time", "lost", "day",
cats" given positive sentiment ($P(\text{term}\vert \text{sentiment} =
\text{Pos})$), and the probability of a positive tweet 
($P(\text{sentiment} = \text{Pos})$).



### Gathering the probabilities

#### Problem

Given the set of positive sentiment tweets:

- Having a fun time.
- Lost track of time, excited half of the day.
- Cats are enjoying themselves.

and the set of negative sentiment tweets:

- Lost the game. Sad day for cats.
- We lost at about half time.
- The half time whistle scared my cats.

Compute the probability of the terms "half", "time",
"lost", "day", cats" given negative sentiment ($P(\text{term}\vert \text{sentiment} =
\text{Neg})$), and the probability of a negative tweet 
($P(\text{sentiment} = \text{Neg})$).

### Computing sentiment

#### Example

From the previous tweets we obtain the probabilities:
\begin{center}
\begin{tabular}{lccccc}
\toprule
& half & time & lost & day & cats \\
\midrule
Positive & 0.33 & 0.67 & 0.33 & 0.33 & 0.33 \\
Negative & 0.67 & 0.67 & 0.67 & 0.33 & 0.67 \\
\bottomrule
\end{tabular}
\end{center}
with $P(\text{sentiment} = \text{Pos}) = 0.5$

Given the tweet "half time day" compute:

- $P(\text{tweet}\vert \text{sentiment} = \text{Pos})$
- $P(\text{tweet}\vert \text{sentiment} = \text{Neg})$
- $P(\text{tweet})$

### Computing sentiment

#### Problem

From the previous tweets we obtain the probabilities:
\begin{center}
\begin{tabular}{lccccc}
\toprule
& half & time & lost & day & cats \\
\midrule
Positive & 0.33 & 0.67 & 0.33 & 0.33 & 0.33 \\
Negative & 0.67 & 0.67 & 0.67 & 0.33 & 0.67 \\
\bottomrule
\end{tabular}
\end{center}
with $P(\text{sentiment} = \text{Pos}) = 0.5$

Given the tweet "lost cats time" compute:

- $P(\text{tweet}\vert \text{sentiment} = \text{Pos})$
- $P(\text{tweet}\vert \text{sentiment} = \text{Neg})$
- $P(\text{tweet})$

### Putting it all together

Preprocessing:

1. Obtain a set of tweets labelled with positive and negative sentiment.
2. Construct the probability table containing 
   $P(\text{term}\vert \text{sentiment} = \text{Pos})$
   and $P(\text{term}\vert \text{sentiment} = \text{Neg})$
3. Record $P(\text{sentiment} = \text{Pos})$

When a new tweet arrives:

1. Compute $P(\text{tweet}\vert \text{sentiment} = \text{Pos})$
2. Compute $P(\text{tweet}\vert \text{sentiment} = \text{Neg})$
3. Compute $P(\text{tweet})$
4. Combine them to obtain $P(\text{sentiment} = \text{Pos} \vert \text{tweet})$
   and $P(\text{sentiment} = \text{Neg} \vert \text{tweet})$.


### Combining the results

Rather than providing both $P(\text{sentiment} = \text{Pos} \vert
\text{tweet})$ and $P(\text{sentiment} = \text{Neg} \vert
\text{tweet})$, we can provide the log likelihood ratio to obtain a sentiment score
$$
\text{score} = \log{\left ( \frac{	
P(\text{sentiment} = \text{Pos} \vert \text{tweet})}{
P(\text{sentiment} = \text{Neg} \vert \text{tweet})}
\right )}$$

- If the score is positive, we predict the sentiment to be positive.
- If the score is negative, we predict the sentiment to be negative.
- If the score is zero, then we are uncertain of the sentiment.

### Computing the sentiment score

#### Problem

Using the previous results and Bayes' theorem:
\begin{align*}
	P(\text{sentiment} = \text{Pos}\vert \text{tweet}) = \frac{P(\text{tweet}\vert \text{sentiment} = \text{Pos})P(\text{sentiment} = \text{Pos})}{P(\text{tweet})}
\end{align*}

Compute:

- $P(\text{sentiment} = \text{Pos} \vert \text{tweet})$
- $P(\text{sentiment} = \text{Neg} \vert \text{tweet})$
- the sentiment score.
- State if the tweet "lost cats time" has positive or negative sentiment.



# Classification Evaluation

### Evaluating a classification model

To evaluate a classifier, we can examine:

- Storage and memory required
- Computation time
- Accuracy of prediction

Of these three, accuracy is usually the most important. If a
classifier is not accurate, then it is useless.

### Evaluating classifier accuracy

A classifier is used to predict the class of objects. To evaluate the classifier, we:

1. Obtain a set of objects that have been manually labelled with their class
2. Compute the class of the objects using the classifier
3. Compare the computed class to the manually labelled class

We want the classifier to perform well for all future predictions.
Therefore, it is important that the data used to evaluate the
classifier is different to the data used to train the classifier.
We call this new data set \alert{validation data}.

### Comparing binary results

The manual and computed set of labels are a sequence of binary values.
Therefore, when comparing them there are four possible combinations

- True Positive (classifier: positive, manual: positive)
- True Negative (classifier: negative, manual: negative)
- False Positive (classifier: positive, manual: negative)
- False Negative (classifier: negative, manual: positive)

Using these, we can compute:

- Sensitivity (True positive rate: of all that are positive, what proportion were classified as positive).
- Specificity (True negative rate: of all that are negative, what proportion were classified as negative).


### Receiver operating characteristic

The Receiver operating characteristic, or ROC curve is a visualisation
of the accuracy of a classifier as its parameters are varied.


```{r, echo=FALSE, fig.width=4.4, fig.height=2.5}
x = c(0,1)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(x,x, type="l", lty=2, xlim=x, ylim=x, xlab="1 - Specificity", ylab = "Sensitivity")
points(0,1)
legend("bottomright", c("Perfect","Random Guess"), pch=c(1,NA), lty=c(0,2))
```



### Evaluating kNN for different $k$



```{r, echo=FALSE, fig.width=4.4, fig.height=2.8}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)

I = I[51:150,]
C = C[51:150]

train.pos = sample(100,20)

I.train = I[train.pos,]
I.test = I[-train.pos,]
C.train = C[train.pos]
C.test = C[-train.pos]

sensitivity = c()
specificity = c()

for (k in 1:9) {
    C.pred = apply(I.test, 1, knn, I.train, C.train, k)

    sensitivity = c(sensitivity, mean(C.pred[C.test == 4] == 4))
    specificity = c(specificity, mean(C.pred[C.test == 3] == 3))
}

plot(1 - specificity, sensitivity, xlim=c(0,1), ylim=c(0,1), pch=as.character(1:9),  xlab="1 - Specificity", ylab = "Sensitivity")
lines(c(0,1), c(0,1), lty=2)
```



### Summary

- Sentiment analysis allows us to identify the sentiment behind text.

- Simple sentiment analysis on tweets allows us to identify if a tweet
  has positive or negative sentiment.

- Simple sentiment analysis can be performed using the machine
  learning method of classification.

- Classification requires us to have a training set containing a
  sample set of tweets and their associated sentiment.

- We examined the Naive Bayes classifier and k Nearest Neighbour
  classifier.

- We also examined how to assess the accuracy of classification using
  Sensitivity and Specificity.


### Next Week

Spatial Analysis


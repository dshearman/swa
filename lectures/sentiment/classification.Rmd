% Sentiment Analysis
% Laurence A. F. Park
% 20th of September 2013

```{r setup, include=FALSE}
opts_chunk$set(dev = 'pdf', fig.cap="")
```


# Sentiment

<http://en.wikipedia.org/wiki/Sentiment_analysis>


# Sentiment Analysis

Classifying the polarity of a piece of text (positive, neutral or
negative).

Advanced sentiment analysis examines the emotion behind the text
(e.g. excited, annoyed, surprised).


## Data Mining Methods

In this unit we have seen:

- Regression: predicting a real value $y$
- Clustering: gathering similar objects into groups

In this lecture we will introduce:

- Classification: predicting an ordinal value from a set.

We will focus on binary classification, meaning the set of ordinal
values is of size 2. These values are usually $\{True, False\}$,
$\{Yes, No\}$, or $\{0,1\}$.


## Classification

Classification is the task of assigning a class (category) to an
object based on its attributes (variables, features).

Classification is a similar task to regression, but regression
predicts a real value for each object.

Classification problems:

- Classify if this email is spam, given the words in the email.
- Classify if a patient has cancer, given the patients attributes.
- Classify if it will rain today, given the humidity of the past few days.
- Classify if there is a tank in this satellite image, given the pixel intensities and positions.
- Classify if a person is suitable for a job, given the contents of their Facebook page.

### Example

Classification:
$$
	[~0.3~1.2~-4.3~2~] \rightarrow \text{Spam}
$$
Regression:
$$
	[~0.3~1.2~-4.3~2~] \rightarrow 4.4
$$


## Classification is Supervised Learning

To classify the class of a data object $x$, we need to compute a
function $c = f(x)$, where $c$ is the class of $x$.

We compute the function $f(\cdot)$ using existing data labelled with
its class. Computing this model based on data is called \alert{training}.

Once we construct the function, we are able to apply the function to
new data to determine its class.


## Common Classification Methods

There are different methods of classification that have different
levels of accuracy and varying computation times. The choice of
classification method depends on the classification problem.

Classification methods include:

- Logistic Regression
- Support Vector Machines
- Neural Networks
- Decision Trees

Each method draws one (or more) line(s) in the data space that best
separates the two classes.

## Classification Example

### Problem: Bran for Breakfast

We surveyed six people and obtained the following information about
their age, gender, hair colour and if they have bran for breakfast.

\begin{center}
\toprule
\begin{tabular}{|A|c|c|c|c|c|}
\midrule
\rowcolor{blue!20}
                          & Age & Gender & Hair Colour & Bran  \\\hline
\firstrowcolour  Person 1 & 7   & Male   & Brown       & No  \\\hline
\secondrowcolour Person 2 & 5   & Female & Black       & No  \\\hline
\thirdrowcolour  Person 3 & 12  & Male   & Black       & No  \\\hline
\fourthrowcolour Person 4 & 32  & Female & Brown       & Yes \\\hline
\fifthrowcolour  Person 5 & 45  & Female & Blonde      & Yes \\\hline
\sixthrowcolour  Person 6 & 28  & Male   & Brown       & Yes \\\hline
\bottomrule
\end{tabular}
\end{center}

Using this data, can we predict if a newly surveyed person has bran
for breakfast based on their age, gender and hair colour?

### Problem: Bran for Dinner

We surveyed six people and obtained the following information about
their age, gender, hair colour and if they have bran for dinner.

\begin{center}
\toprule
\begin{tabular}{|A|c|c|c|c|c|}
\midrule
\rowcolor{blue!20}
                          & Age & Gender & Hair Colour & Bran  \\\hline
\firstrowcolour  Person 1 & 7   & Male   & Brown       & No  \\\hline
\secondrowcolour Person 2 & 5   & Female & Black       & No  \\\hline
\thirdrowcolour  Person 3 & 12  & Male   & Black       & No  \\\hline
\fourthrowcolour Person 4 & 32  & Female & Brown       & No \\\hline
\fifthrowcolour  Person 5 & 45  & Female & Blonde      & Yes \\\hline
\sixthrowcolour  Person 6 & 28  & Male   & Brown       & No \\\hline
\bottomrule
\end{tabular}
\end{center}

Using this data, can we predict if a newly surveyed person has bran
for breakfast based on their age, gender and hair colour?


## Gathering Data for Classification Training

In the previous example, we could have used the classification functions:

- age > 32
- hair colour == blonde
- (age > 32) & (hair colour == blonde)

The model was ambiguous because we had only 1 sample for the ``Bran
for Dinner == Yes'' class.

To obtain a useful model, we generally require:

- as many samples that can be obtained (the more the better)
- balanced sample (similar number of samples for each of the positive
  and negative classes)






## Binary Classification

Classification is to assign a class to an object. Binary
classification is assigning a class to an object where there are only
two classes to choose from. Binary classification is the most common
form of classification, since it is less complex than multi-class
classification, and it allows us to predict answers to Yes/No
(True/False) questions.

### Example

If we want to classify if an email is spam, we would construct a
classifier that takes attributes of an email as its parameters, and
returns True or False:
$$
	\text{is.spam}(\text{email}) \rightarrow \{True, False\}
$$

## Classifier Construction



## Training

## Testing

## Maximum Likelihood


# Classification of Sentiment


It is difficult to identify what part of text sets the sentiment.

Therefore to perform sentiment analysis, we will use a machine
learning to learn from a set of examples and automatically classify
new pieces of text.

Machine learning methods require a set of objects each with a set of
measured features and a class.  We want to examine the features and
make a prediction of the class.  To do this, we obtain many examples
of objects with their classes and learn the relationships between
them.



## Density Estimation

If our training samples cover most of the data space we can use
density estimation to compute the probability of each class in a given
position of the space.

We will be examining the density estimation methods:

- $K$ nearest neighbours
- Naive Bayes



## $K$ Nearest Neighbours

The $K$ Nearest Neighbours classifier is one of the simplest
classifiers. It computes the class of a point as the \alert{mode} of
the classes of the $k$ nearest points from the training set.

The parameter $k$ is a positive integer, and must be chosen based on
the data being used.


## $K$ Nearest Neighbours example

```{r}
plot(cmdscale(dist(iris[,c(1:4)])), col=c(rep(1,50),rep(2,50),rep(3,50)))
```




## $K$ Nearest Neighbours features

$K$ nearest neighbours is considered one of the extreme classifiers,
since its prediction depends on all of the training set, not just
statistics from the training set.

- It requires no training, but does require a training data set.
- All of the computation is performed when predicting the class of a
  point.
- We must store and access all of the training points in order to
  perform prediction.





## Naive Bayes Classifier

If we know the distribution of each class, we can use knowledge of
each distribution and the sample size from each class, to predict the
class label.

### One dimensional example

```{r, echo=FALSE, fig.width=4.4, fig.height=2.7}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
curve(dnorm(x, mean=-2, sd=1), from = -5, to = 5, col = 4)
curve(dnorm(x, mean=2, sd=1), from = -3, to = 3, add=TRUE, col = 2)
```

We have two distributions (they are Normal, but they do not have to
be). If we know the distribution equation for class $C = 1$ and $C =
0$, we have:
$$
	P(X|C = 1) \text{ and } P(X|C = 0)
$$
If we obtain a new sample $x$, we want to compute:
$$
	P(C = 1|X = x) \text{ and } P(C = 0|X = x)
$$
both giving a probability (between 0 and 1). The class with the
largest probability is the most likely class for the sample $x$.

We can also examine:
$$
	L = \log{\left (\frac{P(C = 1|X = x)}{P(C = 0|X = x)}\right )}
$$
If $L > 0$, then $x$ is like to belong to class 1.
If $L < 0$, then $x$ is like to belong to class 0.


## Using Bayes' rule

To perform the classification, we need the distributions:
$$
	P(C = 1|X = x) \text{ and } P(C = 0|X = x)
$$
but we have:
$$
	P(X|C = 1) \text{ and } P(X|C = 0)
$$
Therefore, we use Bayes' rule:
$$
	P(C = 1|X = x)  = \frac{P(X|C = 0)P(C = 0)}{P(X = x)}
$$

- $P(C = 1|X = x)$: Probability class 1, given the sample $x$.
- $P(X = x|C = 1)$: Probability of sampling $x$, given the class of the sample is 1.
- $P(C = 1)$: Probability of a sample being of class 1.
- $P(X = x)$: Probability of sampling the value $x$.


## Log Likelihood Equation

Using Bayes' rule on $L$, we obtain the log likelihood equation:
Giving us:
$$
	L = \log{\left (\frac{P(C = 1|X = x)}{P(C = 0|X = x)}\right )} = 
	\log{\left (\frac{P(X = x|C = 1)P(C = 1)}{P(X = x|C = 0)P(C = 0)}\right )}
$$
which can be separated into:
$$
	L = \log{\left (\frac{P(C = 1)}{P(C = 0)}\right )} + 
	\log{\left (\frac{P(X = x|C = 1)}{P(X = x|C = 0)}\right )}
$$
The first term is based on the probability of sampling from each of
the distributions. If the probabilities are equal $P(C = 1) = P(C =
0)$, then the first term is 0.


## Estimating the Distributions.

In most situations, we will not know the distribution of each
class. Therefore, we will have to estimate the distribution by
sampling from it.

### Estimating the class distribution for class 1

1. Pick a sample size $n$
2. Take a sample of size $n$ from class 1.
3. $P(X = x|C = 1)$ = number of times $x$ occurred in the sample/$n$


## Problem when estimating from continuous distributions

We saw that the probability estimate $P(X = x|C = 1)$ was computed as
the proportion of occurrences of $x$ in a sample from class 1.

If the distribution of $X$ is continuous, then we would not expect to
obtain a sample of exactly $x$, meaning that our estimate of $P(X =
x|C = 1)$ is $0$ for most values of $x$.

```{r, echo=FALSE, fig.width=4.4, fig.height=2.7}
x = rnorm(20)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(x,rep(1,length(x)), type="h")
```

## Creating regions for continuous distributions

To obtain a better estimate of the probability, we must divide the
sample space into equal sized regions (just as when we plot a
histogram). Rather than computing the proportion of $x$ in the sample,
we compute the proportion of the sample that falls in a given region.

The smaller the regions, the closer the estimate will be to the true
probability, but we will require more samples to make sure the many
regions have a chance of being sampled from.

```{r, echo=FALSE, fig.width=4.4, fig.height=2.7}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
hist(x)
```

## Number of samples for distribution estimation

In any situation, the larger the sample size, the better. The number
of samples depends on the number of discrete regions of each variable
being measured.

If each item has a class label (e.g. $C = 1$) and one variable (e.g
age = 28), and the variable can take has 10 possibles values (e.g. 21
to 30), then we need at least a sample size of 10. A sample size of
less than 10 will not cover the 10 possible values for age. A sample
of size 10 will probably not cover the space either, but it might.


## Curse of Dimensionallity

If we have more than one variable




http://en.wikipedia.org/wiki/Curse_of_dimensionality


http://en.wikipedia.org/wiki/Naive_bayes




# Decision Tree

# Evaluation


False Positive Rate
False Negative Rate
True Positive Rate
True Negative Rate


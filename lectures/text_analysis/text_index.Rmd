% Text Analysis
% Laurence A. F. Park
% March 22, 2005

<!-- Setting up R -->
`r opts_chunk$set(warning=FALSE, dev="pdf", fig.cap="", cache=FALSE)`


# Motivation

## Finding the public opinion


The social Web contains the opinions of a large sample of the
population. We can use messages written by the public to examine
public opinion.

[Geek vs Nerd](http://slackprop.wordpress.com/2013/06/03/on-geek-versus-nerd/)


## Digging through the piles of text

\begin{columns}
\begin{column}{0.45\textwidth}
Given ten documents, we could read them and find which are relevant to
our needs. Given a million documents, we have to automate the
process. There is so much text available. How can we find the pieces
that are important?
\end{column}
\begin{column}{0.45\textwidth}
\vspace*{-2em}
\hspace*{-7em}
\otherimagewl{swa_tweets}{10cm}{8cm}
\end{column}
\end{columns}


# Document Models

## The need for a document model

Given a set of documents, we are usually faced with the problem of
searching through them, to either:

- Find the document that is most similar to a query (Google)
- Find the document that is most similar to another document
- Find words that are most similar to another word

### Which tweet is most similar to "Hero memories" or "Game"?

\begin{columns}[t]
\begin{column}{0.45\textwidth}
@Richmond\_FC Final siren, the Tiges win by 41 points at the 
MCG! \#gotiges \#yellowandblack
\end{column}
\begin{column}{0.45\textwidth}
@SJGames \#hook The deities inform the heroes that one of the PCs will
soon die, to be replaced by another version who will retain his
memories. -sjm
\end{column}
\end{columns}

\pause
\vspace{1em}
How can we compute the similarity over millions of tweets?

## Document Models

Documents are defined by their content, the words they contain. 
When we write a document:

- we have model of the document $M_d$ in our mind.
- we then sample words from the document model $M_d$ to create the document.

The words in the document are a sample from a model. We can take many
samples to obtain many documents, each with different words. But
because they all come from the same document model, they contain the
same ideas.

## Document model qualities

In each cases, we need to be able to compare documents to documents,
documents to queries, or words to words, so we need to define a
document and word similarity (or dissimilarity) function:
$$
   s_{i,j} = S(d_i, d_j) 
$$
where $s_{i,j}$ is large if $d_i$ and $d_j$ are similar, and 
$s_{i,j}$ is small if $d_i$ and $d_j$ are dissimilar.

Before we can define a similarity function, we need to define how the
document will be represented. Hence we need to define a mathematical
model for a document, also known as a document model.

## Document models

The most common form of document representation is as a *bag of
words*, meaning that each document is represented as a set of unique
words and an associated weight:

$$
	\text{document} = \{\text{word}_1:\text{weight}, \text{word}_2:\text{weight}, \ldots, \text{word}_n:\text{weight}\}
$$

The weight of each word is a function of the frequency of the word in
the document and in the collection of documents.







## Divergence from Randomness

The importance of a word in a document is inversely proportional to
the probability of it appearing. Meaning, if we expect a word to
appear and it appears, it is not interesting. If we expect a word not
to appear and it doesn't, it is interesting.

Remember that $log(1/x) = -log(x)$, so we can compute the weight of a
term in a document as:
$$
	\text{weight}(w|d) \propto -\log{\left ( P(w|C) \right )}
$$

### Probability of occurrence

Which words do you expect in a document collection from computing:
\begin{center}
	computer, with, nuclear, windows, water, snowman.
\end{center}

\pause

Words with low probability, that appear, are provided a higher weight.

## Using the Binomial distribution

We can compute the probability of a word appearing, using the Binomial
distribution.

\begin{block}{Binomial Distribution}

The probability of $x$ successes, in $n$ trials, where each trial has
the probability of successes $p$.

\end{block}

- A document is a sample of $n$ words. 
- If word $w$ is chosen, we have a success, if word $w$ is not chosen,
  we have a failure.
- The probability $p$ of word $w$ is the proportion of occurrences of
  word $w$ in the document collection (occurrences of word w / total
  number of words in collection).


## Binomial Example

A document collection contains 10 documents, each containing 100 words
(total of 1000 words).  The word "the" appears 100 times in the
collection, so its probability $P(w|C)$ is $p = 100/1000 = 0.1$.  In a
given document of length 100, "the" appears 12 times. The probability
of this occurring is:
$$
\begin{aligned}
	P(X = 12) &= \text{Binomial}(12, n = 100, p = 0.1) \\
	&= {100 \choose 12}0.1^{12}{(1 - 0.1)}^{100 - 12} =  0.0987
\end{aligned}
$$
with weight:
$$
	\text{weight}(w|d) = -log(0.0987) = 2.316
$$

### Problem 

The word "tank" appears 3 times in the collection, so its probability
is $p = 3/1000 = 0.003$. In a given document of length 100, "tank"
appears 2 times. Compute its weight.



## Types of Document Models

A document is a sequence of words. In order to apply mathematical
operations to a set of documents, we must first define a mathematical
model of a document.

There are three main document models:

- Vector Space Model
- Probabilistic Model
- Language Model

Each of these models treats the documents as a *bag of words*, meaning
that order of the words is lost once the model is computed.

## Vector Space Model

Each document is treated as a vector in an $M$ dimensional vector
space, where $M$ is the number of unique terms in the document
collection. Each element of the document vector is associated to a
particular term, the value of the element is the frequency of the term
in the document.

\begin{center}
\begin{tikzpicture}[scale=3]
\coordinate (A) at (0.,0);
\coordinate [label=above:\textcolor{blue}{$\vec{d}_1$}] (E) at (0.5,1);
\coordinate [label=right:\textcolor{blue}{$\vec{d}_2$}] (F) at (1.2,0.9);
\coordinate [label=right:\textcolor{blue}{$\vec{d}_3$}] (G) at (0.8,0.3);

\draw [arrows=->] (A) -- (E);
\draw [arrows=->] (A) -- (F);
\draw [arrows=->] (A) -- (G);


\begin{scope}
  \path[clip] (A) -- (E) -- (G) -- cycle;
  \draw [green, fill=green!20, opacity=.8] (A) circle (15pt);
\end{scope}

\begin{scope}
  \path[clip] (A) -- (E) -- (F) -- cycle;
  \draw [red, fill=red!20, opacity=.8] (A) circle (10pt);
\end{scope}

\end{tikzpicture}
\end{center}

## Document vectors

A document vector $\vec{d}_i$ is an ordered set of term frequencies,
where the order depends on the words in the document collection.


To compare document vectors, we examine the angle between the vectors,
which is inversely proportional to the cosine of the angle:
$$
	s_{i,j} = \cos{(\theta_{i,j})} = \frac{\vec{d}_i \cdot \vec{d}_j}{\norm{\vec{d}_i}\norm{\vec{d}_j}}
$$
where:
$$
	\vec{d}_i \cdot \vec{d}_j = {\sum_{k = 1}^M d_{i,k}d_{j,k}}
	\text{~and~} \norm{\vec{d}_i} = \sqrt{{\sum_{k = 1}^M d_{i,k}^2}}
$$
Therefore, $s_{i,j} = 1$ if $\vec{d}_i = \vec{d}_j$, otherwise $s_{i,j} < 1$.


## Queries

We want to be able to compare queries to documents, in order to find
documents that are most relevant to the query. Fortunately, a query is
a set of words, and can be modelled using a document vector.


## Probabilistic Model

## Language Model


## Example: Constructing a document index

Our document set:

1. Social Web analytics is the best!
2. Social Web analytics is the greatest unit.
3. The best Web unit is Social Web analytics.

\pause

Construct the frequency table:

\begin{center}
\begin{tabular}{lcccccccc}
\toprule
& Social & Web & analytics & is & the & best & greatest & unit \\
\cmidrule(r){1-1}\cmidrule(r){2-9}
$\vec{d}_1$ &      1 &   1 &         1 &  1 &   1 &    1 &        0 &    0 \\
$\vec{d}_2$ &      1 &   1 &         1 &  1 &   1 &    0 &        1 &    1 \\
$\vec{d}_3$ &      1 &   2 &         1 &  1 &   1 &    1 &        0 &    1 \\
\bottomrule
\end{tabular}
\end{center}

Note that each row of the frequency table is an eight dimensional
vector ($M = 8$).

## Example: Query the document index

If we have the query *best Web unit*, we create the query vector $\vec{q}$ in
the eight dimensional vector space:

\begin{center}
\begin{tabular}{lcccccccc}
\toprule
& Social & Web & analytics & is & the & best & greatest & unit \\
\midrule
$\vec{q}$  &   0 &   1 &         0 &  0 &   0 &    1 &        0 &    1 \\
\bottomrule
\end{tabular}
\end{center}

\pause

then compute the similarity of the query to all documents:

\begin{center}
\begin{tabular}{cccc}
\toprule
& $\vec{d}\cdot\vec{q}$ & $\norm{\vec{d}}$ & $S(\vec{d},\vec{q})$ \\
\midrule
$\vec{d}_1$ & 2 & $\sqrt{6}$  & $2/(\sqrt{6}\sqrt{3}) = 0.47$ \\
$\vec{d}_2$ & 2 & $\sqrt{7}$  & $2/(\sqrt{7}\sqrt{3}) = 0.44$ \\
$\vec{d}_3$ & 4 & $\sqrt{10}$ & $4/(\sqrt{10}\sqrt{3}) = 0.73$ \\
$\vec{q}$   &   & $\sqrt{3}$  & \\
\bottomrule
\end{tabular}
\end{center}


## Problem: Index and Query

Build and index of the documents

1. The dog is on the hill

and compute the similarity of the documents to the query:


## Term Weights


One one was a race horse
Two two was one too
One one won one race
Two two won one too



One was a race horse Two too won

# Preprocessing Text

## Preparing for index construction

The English language contains a lot of redundancy that allows us to
express ideas, but provides confusion to computer processes that do not
understand the relationships between the words.

In this section we will examine a few methods to strip down the text
to increase the effectiveness of analysis.

## Stop words

Stop words provide no or only little information to our analysis and
so can be safely removed. Removing stop words can be beneficial to an
analysis, but it always depends on the text being examined.

### Problem

Which words can we remove from these documents:

1. Social Web analytics is the best!
2. Social Web analytics is the greatest unit.
3. The best Web unit is Social Web analytics.


## Stop word list

Here is an example stop word list:

a, able, about, across, after, all, almost, also, am, among, an, and,
any, are, as, at, be, because, been, but, by, can, cannot, could,
dear, did, do, does, either, else, ever, every, for, from, get, got,
had, has, have, he, her, hers, him, his, how, however, i, if, in,
into, is, it, its, just, least, let, like, likely, may, me, might,
most, must, my, neither, no, nor, not, of, off, often, on, only, or,
other, our, own, rather, said, say, says, she, should, since, so,
some, than, that, the, their, them, then, there, these, they, this,
tis, to, too, twas, us, wants, was, we, were, what, when, where,
which, while, who, whom, why, will, with, would, yet, you, your

Lists can be compiled for a specific task.

### When not to remove stop words

By removing stop words, we remove all occurrences of "To be or not to
be."

## Letter case and punctuation

When examining words in documents, we want to identify how often they
appear and in which documents they appear.

To keep all occurrences of a word consistent, we must remove the any
difference in letter case. This is done by adjusting all text to be
lower case.

Punctuation is important for text sequences (sentences), but not
needed when examining individual terms. All punctuation can be
removed.

Our document set has been reduced to:

1. social web analytics best
2. social web analytics greatest unit
3. best web unit social web analytics

## Stemming

There are many words that have the same stem, but are adjusted due to
their use in a sentence. By removing the variation, we obtain a better
understanding the occurrence of the word.

For example the words "fishing", "fished", "fish", and "fisher" have
the stem "fish". The words "argue", "argued", "argues" and "arguing
have the stem "argue".

The most commonly used stemming algorithms is Porter's stemmer.

Our document set has been reduced to:

1. social web analyt best
2. social web analyt great unit
3. best web unit social web analyt

Note that the stems do not have to be words.

# Weighting Terms

## Going beyond term frequency

We have treated the frequency of a term as its weight in the
document. If a word appears two times in a documents, it is twice as
relevant to that document when compared to a document that contains
the word only once.

If a documents contains a word 200 times, is the word twice as
relevant to a document that contains the word 100 times? We would
expect not. As the frequency of a word increase, the relevance of the
word should not increase linearly.

In this section we will examine how to compute the relevance of term
to the document set and to each document.

## Term weights

For a given document collection, we can compute which words are the
most descriptive of the set.

### Requirements for a term weight

- If a term appears in all documents, then is does not provide us with
any information.  
- If a term appears in only one document, then it is
very representative of the difference between that document and the
rest.


## Term weight function

The term weight can be computed using the function:
$$
	w_t = \log_e{\left ( \frac{N}{f_t} \right )}
$$
where $N$ is the number of documents, $f_t$ is the number of
documents containing term $t$, and $w_t$ is the weight of term $t$.

### Problem

Compute the term weights for each of the words in the following
document set:

1. social web analyt best
2. social web analyt great unit
3. best web unit social web analyt

## Term weight plot ($N = 100$)

\vspace{-6em}
```{r, echo=FALSE, fig.width=5, fig.height=4}
plot(1:100, log(100/(1:100)), xlab="Term weight", ylab="Term Document Count")
```

## Within document term weight

The within document term weight is a measure of how important the term
is within the document. This weight is dependent on:

- The frequency of the term in the document
- The importance of the word in the document set (the term weight)


## Within document term weight function

The within document term weight can be computed using:
$$
\begin{aligned}
	w_{d,t} &= \log_e{\left ( f_{d,t} + 1\right )}w_t \\
	 &= \log_e{\left ( f_{d,t} + 1\right )}\log_e{\left ( \frac{N}{f_t} \right )} \\
\end{aligned}
$$
where $f_{d,t}$ is the frequency of term $t$ in document $d$ and
$w_{d,t}$ is the weight of term $t$ in document $d$.

### Problem 

Using the results from the previous problem, compute the within
document term weights for each of the words in the first document:

1. social web analyt best
2. social web analyt great unit
3. best web unit social web analyt


## Weighted frequency plot

As the frequency increases, the increase in weight reduces.

\vspace{-6em}
```{r, echo=FALSE, fig.width=5, fig.height=4}
plot(1:100, log(1:100 + 1), xlab="f", ylab="log(f+ 1)")
```


# Evaluating Results

## Ranking documents

Given a document model, we can compute how similar all documents are
to a particular document, or how similar all documents are to a
query. Either way, the result is a ranked list of documents from best
matching to worst matching.

\begin{center}
\begin{tabular}{lcc}
\toprule
  Document ID & Score & Rank \\
\midrule
  D1012123 & 4.56 & 1 \\
  D2342923 & 4.23 & 2 \\
  D2342233 & 3.78 & 3 \\
  D7453232 & 3.66 & 4 \\
  \vdots & \vdots & \vdots \\
  D6454644 & -4.01 & 983453 \\
\bottomrule
\end{tabular}
\end{center}

The document score is used to provide the rank. Once we have the rank,
we can ignore the score. Google presents the user with the top ten
results, with the option to view the next ten.

## Comparing results for difference document models

If we are given two document models, we can compute two different
ranked lists for the same problem (query). We can use these ranked
lists to evaluate the accuracy of each document model.

To evaluate a document model, we need:

- the truth (the correct ranking)
- an evaluation function (to compare the computed ranking to the correct ranking).


## Obtaining the truth

The ranking of an entire document set for a given query is a
subjective process. Given only 10 documents, there are $10! =
3,628,800$ different possible rankings. If two documents are similar,
it is difficult to say which should be ranked higher than the other.

Rather than providing a ranking as the truth, we can instead provide
the set of documents that are relevant to the query, meaning that the
remaining documents are irrelevant.

A perfect ranking would place all relevant documents higher on the
list than all irrelevant documents. But how do we score ranking that
are not perfect?

### Which ranking is better

\begin{columns}
\begin{column}{0.45\textwidth}
The best ranking is:
\begin{itemize}
\item 1 1 1 1 0 0 0 0
\end{itemize}
\end{column}
\begin{column}{0.45\textwidth}
Which is the better of these two:
\begin{itemize}
\item 1 0 0 0 1 1 1 0
\item 0 1 1 0 1 1 0 0
\end{itemize}
\end{column}
\end{columns}

## Precision at $k$

A simple measure of accuracy is to count the number of relevant
documents in the top $k$ ranked documents.

The precision at 5 for the following ranking is 2:
\begin{center}
	1 0 0 1 0 1 1 0 1 1
\end{center}

### Problem

Calculate the precision at 5 for these rankings:

- 0 0 1 1 1 0 1 1 0 0
- 1 0 0 0 0 0 0 1 0 1
- 1 1 1 1 1 1 1 1 0 1

## Reciprocal Rank

In some cases, we are interested on returning one relevant document,
rather then a number of them. For this case, the position of the
relevant document is important.

The reciprocal rank is the reciprocal of the rank of the first
relevant document. If the first relevant document appears at rank $x$,
the reciprocal rank is $1/x$.

### Problem

Calculate the reciprocal rank for the following rankings:

- 0 0 1 1 1 0 1 1 0 0
- 1 0 0 0 0 0 0 1 0 1
- 1 1 1 1 1 1 1 1 0 1


## Average Precision

If we want to measure how well we ranked all documents, we use Average
Precision. This is the average of the precision at each relevant
document:

$$
	\text{AP} = \frac{1}{R}{\sum_{i = 1}^{R} \frac{i}{r_i}}
$$
where $r_i$ is the rank is the $i$th top ranked relevant document.

### Example

Given the ranked list with relevant documents in the positions: 

1 1 0 0 0 1

$AP = (1/1 + 2/2 + 3/6)/3 = 0.833$


## Summary

You should know how to:

- construct a text index
- represent documents and queries as vectors
- find the terms weight and within document term weight
- evaluate the accuracy of search results using Precision at $k$,
  Reciprocal Rank and Average Precision.






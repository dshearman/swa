% Text Analysis
% Laurence A. F. Park
% March 22, 2005



# Motivation

## Finding the public opinion


The social Web contains the opinions of a large sample of the
population. We can use messages written by the public to examine
public opinion.

To examine opinion, we must examine the text and be able to compute:

- the relationship between words (dog is more similar to puppy than rocket)
- the relationship between words and documents (a document is related to the word puppy)
- the relationship between documents (document $A$ is more similar to document $B$ than $C$)

Example using word similarities:
[Geek vs Nerd](http://slackprop.wordpress.com/2013/06/03/on-geek-versus-nerd/)


## Digging through the piles of text

\begin{columns}
\begin{column}{0.45\textwidth}
Given ten documents, we could read them and record the relationships between words and documents.

Given a million documents, we have to automate the process.
\end{column}
\begin{column}{0.45\textwidth}
\vspace*{-2em}
\hspace*{-7em}
\otherimagewl{swa_tweets}{10cm}{8cm}
\end{column}
\end{columns}


# Document Models

## The need for a document model

Given a set of documents, we are usually faced with the problem of
searching through them, to either:

- Find the document that is most similar to a query (Google)
- Find the document that is most similar to another document
- Find words that are most similar to another word

### Which tweet is most similar to "Hero memories" or "Game"?

\begin{columns}[t]
\begin{column}{0.45\textwidth}
@Richmond\_FC Final siren, the Tiges win by 41 points at the 
MCG! \#gotiges \#yellowandblack
\end{column}
\begin{column}{0.45\textwidth}
@SJGames \#hook The deities inform the heroes that one of the PCs will
soon die, to be replaced by another version who will retain his
memories. -sjm
\end{column}
\end{columns}

\pause
\vspace{1em}
How can we compute the similarity over millions of tweets?

## Document Models

Documents are defined by their content, the words they contain. 
When we write a document:

- we have model of the document $M_d$ in our mind.
- we then sample words from the document model $M_d$ to create the document.

The words in the document are a sample from a model. We can take many
samples to obtain many documents, each with different words. But
because they all come from the same document model, they contain the
same ideas.


## Document model form

The most common form of document representation is as a *bag of
words*, meaning that each document is represented as a set of unique
words and an associated weight:

$$
	\text{document} = \{\text{word}_1:\text{weight}, \text{word}_2:\text{weight}, \ldots, \text{word}_n:\text{weight}\}
$$

The weight of each word is a function of the frequency of the word in
the document and in the collection of documents.



## Types of Document Models


There are three main document models:

- Vector Space Model
- Probabilistic Model
- Language Model

Each of these models treats the documents as a *bag of words*, meaning
that order of the words is lost once the model is computed.

We will examine the Vector Space Model.

## Vector Space Model

Each document is treated as a vector in an $M$ dimensional vector
space, where $M$ is the number of unique terms in the document
collection. Each element of the document vector is associated to a
particular term, the value of the element is the frequency of the term
in the document.

\begin{center}
\begin{tikzpicture}[scale=3]
\coordinate (A) at (0.,0);
\coordinate [label=above:\textcolor{blue}{$\vec{d}_1$}] (E) at (0.5,1);
\coordinate [label=right:\textcolor{blue}{$\vec{d}_2$}] (F) at (1.2,0.9);
\coordinate [label=right:\textcolor{blue}{$\vec{d}_3$}] (G) at (0.8,0.3);

\draw [arrows=->] (A) -- (E);
\draw [arrows=->] (A) -- (F);
\draw [arrows=->] (A) -- (G);


\begin{scope}
  \path[clip] (A) -- (E) -- (G) -- cycle;
  \draw [green, fill=green!20, opacity=.8] (A) circle (15pt);
\end{scope}

\begin{scope}
  \path[clip] (A) -- (E) -- (F) -- cycle;
  \draw [red, fill=red!20, opacity=.8] (A) circle (10pt);
\end{scope}

\end{tikzpicture}
\end{center}

## Document vectors

A document vector $\vec{d}_i$ is an ordered set of term frequencies
$f_{d,t}$, where the order depends on the words in the document
collection.


To compare document vectors, we examine the angle between the vectors,
which is inversely proportional to the cosine of the angle:
$$
	s({\vec{d}_i,\vec{d}_j}) = \cos{(\theta_{i,j})} = \frac{\vec{d}_i \cdot \vec{d}_j}{\norm{\vec{d}_i}\norm{\vec{d}_j}}
$$
where:
$$
	\vec{d}_i \cdot \vec{d}_j = {\sum_{k = 1}^M f_{d_i,t_k}f_{d_j,t_k}}
	\text{~and~} \norm{\vec{d}_i} = \sqrt{{\sum_{k = 1}^M f_{d_i,t_k}^2}}
$$
Therefore, $s({\vec{d}_i,\vec{d}_j}) = 1$ if $\vec{d}_i/\norm{\vec{d}_i} = \vec{d}_j/\norm{\vec{d}_j}$, otherwise $-1 \le s_{i,j} < 1$.


## Queries

Using the Vector Space Model, we can compare documents to documents,
but we want to be able to compare queries to documents as well (to find the documents most relevant to a query).

Fortunately, a query is a set of words, and can be seen as a very
short document. Therefore, we can represent a query using a document
vector.


## Example: Document vectors in the Vector Space

Our document set:

1. Social Web analytics is the best!
2. Social Web analytics is the greatest unit.
3. The best Web unit is Social Web analytics.

\pause

Construct the frequency table:

\begin{center}
\begin{tabular}{lcccccccc}
\toprule
\onslide<2->{& Social & Web & analytics & is & the & best & greatest & unit} \\
\cmidrule(r){1-1}\cmidrule(r){2-9}
\onslide<3->{$\vec{d}_1$ &      1 &   1 &         1 &  1 &   1 &    1 &        0 &    0} \\
\onslide<4->{$\vec{d}_2$ &      1 &   1 &         1 &  1 &   1 &    0 &        1 &    1} \\
\onslide<5>{$\vec{d}_3$ &      1 &   2 &         1 &  1 &   1 &    1 &        0 &    1} \\
\bottomrule
\end{tabular}
\end{center}

Note that each row of the frequency table is an eight dimensional
vector ($M = 8$).

## Example: Query the document index

If we have the query *best Web unit*, we create the query vector $\vec{q}$ in
the eight dimensional vector space:

\begin{center}
\begin{tabular}{lcccccccc}
\toprule
& Social & Web & analytics & is & the & best & greatest & unit \\
\midrule
$\vec{q}$  &   0 &   1 &         0 &  0 &   0 &    1 &        0 &    1 \\
\bottomrule
\end{tabular}
\end{center}

\pause

then compute the similarity of the query to all documents:

\begin{center}
\begin{tabular}{cccc}
\toprule
& $\vec{d}\cdot\vec{q}$ & $\norm{\vec{d}}$ & $S(\vec{d},\vec{q})$ \\
\midrule
$\vec{d}_1$ & 2 & $\sqrt{6}$  & \onslide<3->{$2/(\sqrt{6}\sqrt{3}) = 0.47$} \\
$\vec{d}_2$ & 2 & $\sqrt{7}$  & \onslide<4->{$2/(\sqrt{7}\sqrt{3}) = 0.44$} \\
$\vec{d}_3$ & 4 & $\sqrt{10}$ & \onslide<5>{$4/(\sqrt{10}\sqrt{3}) = 0.73$} \\
$\vec{q}$   &   & $\sqrt{3}$  & \\
\bottomrule
\end{tabular}
\end{center}


## Problem: Index and Query

### Problem

Construct the frequency table of the following four documents:

- One one was a race horse
- Two two was one too
- One one won one race
- Two two won one too

and compute the similarity of the documents to the query: "one won"


## Document Matrix

Computing the similarity is the inner product of normalised
vectors. Therefore, we can compute the similarity to all documents
using a matrix multiply.

Our document matrix is:
$$
	D = 
	\left [
	\begin{array}{c}
	\vec{d_1}/\norm{d_1} \\
	\vec{d_2}/\norm{d_2} \\
	\vdots \\
	\vec{d_N}/\norm{d_N} \\
	\end{array}
	\right ]
	=
	\left [
	\begin{array}{cccc}
	w_{d_1,t_1} & w_{d_1,t_2} & \ldots & w_{d_1,t_M} \\
	w_{d_2,t_1} & w_{d_2,t_2} & \ldots & w_{d_2,t_M} \\
	\vdots & \vdots & \ddots & \vdots \\
	w_{d_N,t_1} & w_{d_N,t_2} & \ldots & w_{d_N,t_M} \\
	\end{array}
	\right ]
$$
where:
$$
	w_{d_i,t_j} = f_{d_i,t_j}/\norm{d_i}
$$
and $w_{d_i,t_j}$ and $f_{d_i,t_j}$ are the weight and term frequency
of word $t_j$ in document $d_i$.


## Similarity using the Document Matrix

Given a document matrix $D$, we can compute the similarity of all
documents to query $\vec{q}$ using:
$$
	\vec{s} = D\vec{q}^{T}
$$
where $\vec{s}$ is the vector of similarity scores, and 
$\vec{q}^{T}$ is the transpose of $\vec{q}$.

# Word Weight

## Importance of words

Not all words have equal importance, so they should not contribute
equally to the similarity score. For example, if a word appears in
every document, it is not very informative. If a word appears in one
document, it may define the document.

The weight of a word depends on its context, therefore, we must
compute the weight of each word when given a new document set.

\begin{block}{Example}
The word "blood" will have the weight:
\begin{itemize}
\item ~[low] Medical research article set
\item ~[high] My brother's diary.
\end{itemize}
\end{block}


## Going beyond term frequency

We have treated the normalised frequency of a term as its weight
$w_{d,t}$ in the document. If a word appears two times in a documents,
it is twice as relevant to that document when compared to a document
that contains the word only once.

If a document contains a word 200 times, is the word twice as
relevant to a document that contains the word 100 times? We would
expect not. As the frequency of a word increase, the relevance of the
word should not increase linearly.

In this section we will examine how to compute the relevance of term
to the document set and to each document.



## Computing Word Weight

When computing word weight (the importance of a word in the document
set), we must take into account the distribution of the words.

We will examine the weighting methods:

- Divergence from Randomness
- TF-IDF


## Divergence from Randomness

The importance of a word in a document is inversely proportional to
the probability of it appearing. Meaning, if we expect a word to
appear and it appears, it is not interesting. If we expect a word not
to appear and it does, it is interesting.

Remember that $log(1/x) = -log(x)$, so we can compute the weight of
word $w$ in a document $d$ as:
$$
	w_{d,t} = -\log{\left ( P(w|d) \right )}
$$

\pause

### Probability of occurrence

Given a set of computing documents, which words should have high weight?
\begin{center}
	computer, with, nuclear, windows, water, snowman.
\end{center}

\pause

Words with low probability, that appear, are provided a higher weight.


## The effect of $-log(x)$

At the probability increases, $-\log{p}$ decreases:
\vspace{-6em}

```{r, echo=FALSE, fig.width=5, fig.height=4, dev="pdf"}
x = (1:100)/100
plot(x, -log(x), xlab="P(w|d)", ylab="-log(P(w|d))", type="l")
```

## Using the Binomial distribution

We can compute the probability of a word appearing, using the Binomial
distribution.

\begin{block}{Binomial Distribution}

The probability of $x$ successes, in $n$ trials, where each trial has
the probability of successes $p$.

\end{block}

- A document is a sample of $n$ words. 
- If word $w$ is chosen, we have a success, if word $w$ is not chosen,
  we have a failure.
- The probability $p$ of word $w$ is the proportion of occurrences of
  word $w$ in the document collection (occurrences of word $w$ / total
  number of words in collection).


## Binomial Example

A document collection contains 10 documents, each containing 100 words
(total of 1000 words).  The word "the" appears 100 times in the
collection, so its probability $P(w|C)$ is $p = 100/1000 = 0.1$.  In a
given document of length 100, "the" appears 12 times. The probability
of this occurring is:
$$
\begin{aligned}
	P(X = 12) &= \text{Binomial}(12, n = 100, p = 0.1) \\
	&= {100 \choose 12}0.1^{12}{(1 - 0.1)}^{100 - 12} =  0.0987
\end{aligned}
$$
with weight:
$$
	w_{d,t} = -log(0.0987) = 2.316
$$


## DFR Binomial Horses


\begin{block}{Problem}

Compute the DFR Binomial weight for each word in the first document
from the document set below.

\begin{itemize}
\item One one was a race horse
\item Two two was one too
\item One one won one race
\item Two two won one too
\end{itemize}

\end{block}

Hint: the Binomial probability is computed as:
$$
\begin{aligned}
	P(X = x) &= {n \choose x}p^{x}{(1 - p)}^{n - x}
\end{aligned}
$$


## TF-IDF

TF-IDF weighting was designed using experimentation, and has shown to
provide good results for a simple method.

- TF: term frequency
- IDF: Inverse Document Frequency

The weight for term in a document is computed as the product of the
term frequency weight and inverse document frequency weight.


## Term weight function (IDF)

IDF is a measure of the terms importance across the document collection.

The term weight can be computed using the function:
$$
	\text{IDF}_t = \log_e{\left ( \frac{N}{f_t} \right )}
$$
where $N$ is the number of documents, $f_t$ is the number of
documents containing term $t$, and $\text{IDF}_t$ is the weight of term $t$.



## Term weight plot ($N = 100$)

As the term document count increases, its weight decreases.
\vspace{-6em}

```{r, echo=FALSE, fig.width=5, fig.height=3.8, dev="pdf"}
plot(1:100, log(100/(1:100)), ylab="Term weight", xlab="Term Document Count")
```

## Computing the IDF

### Problem 

Compute the IDF term weights for each of the words in the following
document set:

- One one was a race horse
- Two two was one too
- One one won one race
- Two two won one too


## Within document term weight (TF)

The within document term weight (TF) is a measure of how important the
term is within the document. This weight is dependent on the frequency
of the term in the document.

The within document term weight can be computed using:
$$
\begin{aligned}
	\text{TF}_{d,t} &= \log_e{\left ( f_{d,t} + 1\right )}
\end{aligned}
$$
where $f_{d,t}$ is the frequency of term $t$ in document $d$ and
$\text{TF}_{d,t}$ is the weight of term $t$ in document $d$.

## TF weighted frequency plot

As the frequency increases, the increase in weight reduces.
\vspace{-6em}

```{r, echo=FALSE, fig.width=5, fig.height=3.8, dev="pdf"}
plot(1:100, log(1:100 + 1), xlab="Term Frequency", ylab="log(Term Frequency + 1)")
```


## TF-IDF: Putting it together.

The weighted term frequency is given as:
$$
\begin{aligned}
	w_{d,t} &= \text{TF}_t\times \text{IDF}_{d,t} \\
		&= \log_e{\left ( f_{d,t} + 1\right )}\log_e{\left ( \frac{N}{f_t} \right )} \\
\end{aligned}
$$
where $f_{d,t}$ is the frequency of term $t$ in document $d$ and
$w_{d,t}$ is the weight of term $t$ in document $d$.

### Problem 

Using the results from the previous problem, compute the within
document term weights for each of the words in the first document:

- One one was a race horse
- Two two was one too
- One one won one race
- Two two won one too









# Preprocessing Text

## Preparing for index construction

The English language contains a lot of redundancy that allows us to
express ideas, but provides confusion to computer processes that do not
understand the relationships between the words.

In this section we will examine a few methods to strip down the text
to increase the effectiveness of analysis.

## Stop words

Stop words provide no or only little information to our analysis and
so can be safely removed. Removing stop words can be beneficial to an
analysis, but it always depends on the text being examined.

### Problem

Which words can we remove from these documents:

1. Social Web analytics is the best!
2. Social Web analytics is the greatest unit.
3. The best Web unit is Social Web analytics.


## Stop word list

Here is an example stop word list:

a, able, about, across, after, all, almost, also, am, among, an, and,
any, are, as, at, be, because, been, but, by, can, cannot, could,
dear, did, do, does, either, else, ever, every, for, from, get, got,
had, has, have, he, her, hers, him, his, how, however, i, if, in,
into, is, it, its, just, least, let, like, likely, may, me, might,
most, must, my, neither, no, nor, not, of, off, often, on, only, or,
other, our, own, rather, said, say, says, she, should, since, so,
some, than, that, the, their, them, then, there, these, they, this,
tis, to, too, twas, us, wants, was, we, were, what, when, where,
which, while, who, whom, why, will, with, would, yet, you, your

Lists can be compiled for a specific task.

\pause 

### When not to remove stop words

By removing stop words, we remove all occurrences of "To be or not to
be."

## Letter case and punctuation

When examining words in documents, we want to identify how often they
appear and in which documents they appear.

To keep all occurrences of a word consistent, we must remove any
difference in letter case. This is done by adjusting all text to be
lower case.

Punctuation is important for text sequences (sentences), but not
needed when examining individual terms. All punctuation can be
removed.

Our document set has been reduced to:

1. social web analytics best
2. social web analytics greatest unit
3. best web unit social web analytics

## Stemming

There are many words that have the same stem, but are adjusted due to
their use in a sentence. By removing the variation, we obtain a better
understanding the occurrence of the word.

For example the words "fishing", "fished", "fish", and "fisher" have
the stem "fish". The words "argue", "argued", "argues" and "arguing"
have the stem "argue".

The most commonly used stemming algorithm is Porter's stemmer.

Our document set has been reduced to:

1. social web analyt best
2. social web analyt great unit
3. best web unit social web analyt

Note that the stems do not have to be words.







## Summary

You should know how to:

- construct document and query vectors, and use them to compute similarity
- compute term weights using DFR and TF-IDF
- preprocess a document collection


## Next Week

Graphs



% Text Mining 2: Clustering
% Laurence A. F. Park
% March 22, 2005


```{r setup, include=FALSE}
opts_chunk$set(dev = 'pdf', fig.cap="", fig.width=4.4, fig.height=3.1)
library("mvtnorm")
```

## Motivation

We have 1000 tweets. What are they about? Can we summarise them?

Clustering allows us to group similar documents or words, allowing us
to identify topics in the document set.

# Introduction to Clustering

## What is Clustering?

### Clustering

To gather objects into clusters.

### Cluster

A subset where each element of the subset is similar using some
measurement and each element is dissimilar to element of other
clusters.

\vspace{1em}
Therefore, for a given clustering, items in a cluster should be close,
but items in different clusters should be distant.

## Simple Clustering Example

\newenvironment{latexcode}{}{}
\begin{latexcode}
\definecolor{clusterone}{rgb}{1,0.5,0.5}
\definecolor{clustertwo}{rgb}{0.5,0.5,1}
\definecolor{clusterthree}{rgb}{0.5,1,1}

\newcommand{\firstrowcolour}{}
\newcommand{\secondrowcolour}{}
\newcommand{\thirdrowcolour}{}
\newcommand{\fourthrowcolour}{}
\newcommand{\fifthrowcolour}{}
\newcommand{\sixthrowcolour}{}
\only<2>{
  We can cluster by age:
  \renewcommand{\firstrowcolour}{\rowcolor{clusterone}}
  \renewcommand{\secondrowcolour}{\rowcolor{clusterone}}
  \renewcommand{\thirdrowcolour}{\rowcolor{clusterone}}
  \renewcommand{\fourthrowcolour}{\rowcolor{clustertwo}}
  \renewcommand{\fifthrowcolour}{\rowcolor{clustertwo}}
  \renewcommand{\sixthrowcolour}{\rowcolor{clustertwo}}
}
\only<3>{
  We can cluster by gender:
  \renewcommand{\firstrowcolour}{\rowcolor{clusterone}}
  \renewcommand{\secondrowcolour}{\rowcolor{clustertwo}}
  \renewcommand{\thirdrowcolour}{\rowcolor{clusterone}}
  \renewcommand{\fourthrowcolour}{\rowcolor{clustertwo}}
  \renewcommand{\fifthrowcolour}{\rowcolor{clustertwo}}
  \renewcommand{\sixthrowcolour}{\rowcolor{clusterone}}
}
\only<4>{
  We can cluster by hair colour:
  \renewcommand{\firstrowcolour}{\rowcolor{clusterone}}
  \renewcommand{\secondrowcolour}{\rowcolor{clustertwo}}
  \renewcommand{\thirdrowcolour}{\rowcolor{clustertwo}}
  \renewcommand{\fourthrowcolour}{\rowcolor{clusterone}}
  \renewcommand{\fifthrowcolour}{\rowcolor{clusterthree}}
  \renewcommand{\sixthrowcolour}{\rowcolor{clusterone}}
}
\only<5>{
  We can cluster by age and hair colour (here we have assumed that brown is more similar to blonde than black):
  \renewcommand{\firstrowcolour}{\rowcolor{clusterone}}
  \renewcommand{\secondrowcolour}{\rowcolor{clustertwo}}
  \renewcommand{\thirdrowcolour}{\rowcolor{clustertwo}}
  \renewcommand{\fourthrowcolour}{\rowcolor{clusterthree}}
  \renewcommand{\fifthrowcolour}{\rowcolor{clusterthree}}
  \renewcommand{\sixthrowcolour}{\rowcolor{clusterthree}}
}

\begin{center}
%\only<1-6>{
%\newcolumntype{A}{c}
\newcolumntype{A}{>{\columncolor{blue!20}}c}
\begin{tabular}{|A|c|c|c|c|c|}
\hline
\rowcolor{blue!20}
                          & Age & Gender & Hair Colour \\\hline
\firstrowcolour  Person 1 & 7   & Male   & Brown  \\\hline
\secondrowcolour Person 2 & 5   & Female & Black  \\\hline
\thirdrowcolour  Person 3 & 12  & Male   & Black  \\\hline
\fourthrowcolour Person 4 & 32  & Female & Brown  \\\hline
\fifthrowcolour  Person 5 & 45  & Female & Blonde \\\hline
\sixthrowcolour  Person 6 & 28  & Male   & Brown  \\\hline
\end{tabular}
\end{center}
\end{latexcode}

## Clustering by observation

Clusters may be identifiable for low dimensional data, or data projected to two dimensions using Multidimensional Scaling:

```{r, echo=FALSE, fig.width=4.4, fig.height=2.7}
cluster1 = rmvnorm(30, mean = c(7,5), sigma = matrix(c(1,0,0,1),2,2))
cluster2 = rmvnorm(30, mean = c(2,12), sigma = matrix(c(1,0,0,1),2,2))
cluster3 = rmvnorm(30, mean = c(15,12), sigma = matrix(c(1,0,0,1),2,2))
cluster4 = rmvnorm(30, mean = c(14,3), sigma = matrix(c(1,0,0,1),2,2))
X.k = rbind(cluster1,cluster2,cluster3,cluster4)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.k[,1], X.k[,2], xlab=expression("x"[1]), ylab=expression("x"[2]))
```

## Clustering Text

Unfortunately, text data is very high dimensional and may be difficult
to visualise. We saw in the Text Mining 1 lecture that we can
represent a text document (or tweet) using an $M$ dimensional vector:
$$
  \vec{d}_i = [ w_{d_it_1} ~ w_{d_it_2} ~ w_{d_it_3} ~ \ldots ~ w_{d_it_M} ]
$$
where $M$ is the number of unique words in the document collection,
and $w_{d_it_j}$ is the weight of term $t_j$ in document $d_i$.  We
also saw that the set of document vectors can be combined to form the
document-term matrix $A$.

To cluster these vectors, we need to be able to compute which vectors
are similar and which are dissimilar. We do this by defining a
similarity metric for the document vectors. 


## Similarity for Document Vectors

In this Unit, we have seen the dissimilarity functions:

- Euclidean Distance (Text Mining 1, Visualisation)
- Binary metric (Visualisation)

and the similarity function:

- Cosine measure (Text Mining 1)



### Similarity vs. Dissimilarity

Dissimilarity measures how different items are, so a dissimilarity of
0 means the items are the same. As the difference in the items
increases, the dissimilarity increases.

Similarity measures how similar items are, so a similarity of 0 means
the items are different.  The more similar the items are, the greater
the similarity score. The Cosine measure has a maximum of 1, meaning
that if two items are the same, the similarity score is 1.

## Computing Clusters

Once we have a vector space and a (dis)similarity function for the
vectors, we can use a clustering algorithm to cluster the vectors.

In this Unit, we will examine the clustering algorithms:

- K-means clustering
- Hierarchical clustering


# K-means Clustering

## K-means

K-means clustering was designed to be used in a Euclidean space,
meaning it uses the Euclidean distance:
$$
	d(\vec{x}_i,\vec{x}_j) = {\sum_{n=1}^{N} {(x_{in} - x_{jn})}^2} = \norm{\vec{x}_i - \vec{x}_j}_2^2
$$


We want to minimise the within-point scatter over all clusters:
$$
	W(C) = {\sum_{k = 1}^K {\sum_{C(i) = k} {\sum_{C(j) = k} \norm{\vec{x}_i - \vec{x}_j}_2^2}}}
$$
$K$ is the number of clusters, $C(i) = k$ if point $\vec{x}_i$ belongs to cluster $k$, and $W(C)$ is the
sum of the distances between all points in each cluster. By minimising $W(C)$, we obtain tight clusters.


## K-means Algorithm

K-means iterates through two steps, until the centres stabilise (stop moving).

### Compute which object belongs to which cluster

For each object $\vec{x}_i$:
$$
	C(i) = \argmin_{1 \le k \le K}{\norm{\vec{x}_i - \vec{m}_k}}
$$

### Compute the centre of each cluster

For each cluster $k$:
$$
\begin{aligned}
	\vec{m}_k &= \argmin_{\vec{m} \in \mathbb{R}^m}{\sum_{C(i) = k} \norm{\vec{x}_i - \vec{m}}}\\
	    &= \frac{1}{N_k}{\sum_{C(i) = k} \vec{x}_i}
\end{aligned}
$$

## Complete K-means Algorithm

1. Choose $K$ the number of clusters wanted.
2. Assign the cluster centres $\vec{m}_k$ randomly.
3. While the new centres are different to the last centres:
    i. Assign each object to its closest cluster centre.
    ii. Compute the cluster mean from the cluster objects.

## K-means Example: Assign Centres

```{r, echo=FALSE}
cluster1 = rmvnorm(30, mean = c(7,5), sigma = matrix(c(1,0,0,1),2,2))
cluster2 = rmvnorm(30, mean = c(2,12), sigma = matrix(c(1,0,0,1),2,2))
cluster3 = rmvnorm(30, mean = c(15,12), sigma = matrix(c(1,0,0,1),2,2))
cluster4 = rmvnorm(30, mean = c(14,3), sigma = matrix(c(1,0,0,1),2,2))
X.k = rbind(cluster1,cluster2,cluster3,cluster4)
N = nrow(X.k)
clusters = sample(4, N, replace=TRUE)
compute.means = function(X.k, clusters) {
    mean1 = apply(X.k[clusters == 1,],2,mean)
    mean2 = apply(X.k[clusters == 2,],2,mean)
    mean3 = apply(X.k[clusters == 3,],2,mean)
    mean4 = apply(X.k[clusters == 4,],2,mean)
    return(rbind(mean1, mean2, mean3, mean4))
}
means = compute.means(X.k, clusters)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.k[,1], X.k[,2], xlab=expression("x"[1]), ylab=expression("x"[2]))
points(means[,1], means[,2], pch=22, bg=c(1:4))
```



## K-means Example: Assign cluster membership

```{r, echo=FALSE}
cluster.distance = function(x, means) {
    apply(means, 1, point.distance, x)
}
point.distance = function(x, y) {
    return(sum((x - y)^2))
}
cluster.membership = function(X.k, means) {
    return(apply(apply(X.k, 1, cluster.distance, means),2,which.min))
}
clusters = cluster.membership(X.k, means)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.k[,1], X.k[,2], xlab=expression("x"[1]), ylab=expression("x"[2]), col=clusters)
points(means[,1], means[,2], pch=22, bg=c(1:4))
```

## K-means Example: Recompute cluster centres

```{r, echo=FALSE}
means = compute.means(X.k, clusters)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.k[,1], X.k[,2], xlab=expression("x"[1]), ylab=expression("x"[2]), col=clusters)
points(means[,1], means[,2], pch=22, bg=c(1:4))
```

## K-means Example: Reassign cluster membership

```{r, echo=FALSE}
clusters = cluster.membership(X.k, means)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.k[,1], X.k[,2], xlab=expression("x"[1]), ylab=expression("x"[2]), col=clusters)
points(means[,1], means[,2], pch=22, bg=c(1:4))
```

## K-means Example: Recompute cluster centres

```{r, echo=FALSE}
means = compute.means(X.k, clusters)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.k[,1], X.k[,2], xlab=expression("x"[1]), ylab=expression("x"[2]), col=clusters)
points(means[,1], means[,2], pch=22, bg=c(1:4))
```

## K-means numerical Example

Five two dimensional data points. Let's find two clusters.

```{r, echo=FALSE}
X = matrix(c(1,1,2,4,4,1,2,1,4,5),5,2)
print(X)
```
\pause

### Problem

Find 3 clusters using the initial centres:
```{r, echo=FALSE}
centers = matrix(c(1,2,3,1,2,3),3,2)
print(centers)
```


## Problems with K-means

K-means clustering is one of the most widely used clustering
algorithms due to its simplicity. But it does have problems.

- K means requires the cluster centres to allocated before the
  iteration begins. The initial cluster centres are chosen randomly,
  so there is a chance that we obtain different clusters each time we
  run K-means.

- K-means uses Euclidean distance, which may not be appropriate for
  us. We will next examine Spherical K-means to use Cosine similarity.

- K-means requires us to set the number of clusters before the
  algorithm is run, but we usually don't know how many clusters there
  are. We will later examine how to set the number of clusters.



## Spherical K-means Algorithm

We may want to use K-means with the Cosine similarity, rather than
Euclidean distance (remember that Cosine similarity measures the
Cosine of the angle between the vectors). If we normalise the points
($\vec{z}_i = \vec{x}_i/\norm{\vec{x}_i}$):

### Compute which object belongs to which cluster

For each object $\vec{x}_i$:
$$
	C(i) = \argmin_{1 \le k \le K}{\frac{\vec{x}_i \cdot \vec{m}_k}{\norm{\vec{x}_i}\norm{\vec{m}_k}}}
	= \argmin_{1 \le k \le K}{\vec{z}_i \cdot \vec{m}_k}
$$

### Compute the centre of each cluster

For each cluster $k$:
$$
	\vec{m}_k = \frac{\sum_{C(i) = k} \vec{z}_i}{\norm{\sum_{C(i) = k} \vec{z}_i}}
$$


# Choosing the number of clusters

## Number of Clusters

What makes a good clustering?

```{r, echo=FALSE}
cluster1 = rmvnorm(30, mean = c(7,5), sigma = matrix(c(1,0,0,1),2,2))
cluster2 = rmvnorm(30, mean = c(2,12), sigma = matrix(c(1,0,0,1),2,2))
cluster3 = rmvnorm(30, mean = c(15,12), sigma = matrix(c(1,0,0,1),2,2))
cluster4 = rmvnorm(30, mean = c(14,3), sigma = matrix(c(1,0,0,1),2,2))
X.large = rbind(cluster1,cluster2,cluster3,cluster4)
#par(mar = (c(3.5, 3.5, 4, 2) + 0.1))
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.large[,1], X.large[,2], xlab=expression("x"[1]), ylab=expression("x"[2]),)
```

## Choosing the number of Clusters

K-means requires us to set the number of clusters before we run the
algorithm.

To decide on the number of clusters, we must remember that a good clustering will have:

- all points within a cluster are close
- all points in different clusters are distant.

To measure the closeness of points within and between clusters, we will use:

- SST: Total Sum of Squares
- SSW: Within Sum of Squares
- SSB: Between Sum of Squares



## Total Sum of Squares (SST)

```{r, echo=FALSE}
cluster1 = rmvnorm(5, mean = c(5,5), sigma = matrix(c(1,0,0,1),2,2))
cluster2 = rmvnorm(5, mean = c(2,12), sigma = matrix(c(1,0,0,1),2,2))
cluster3 = rmvnorm(5, mean = c(15,9), sigma = matrix(c(1,0,0,1),2,2))
cluster4 = rmvnorm(5, mean = c(14,3), sigma = matrix(c(1,0,0,1),2,2))
cluster.number = c(rep(1,5),rep(2,5),rep(3,5),rep(4,5))
X = rbind(cluster1,cluster2,cluster3,cluster4)
m = apply(X,2,mean)
point.count = nrow(X)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X[,1], X[,2], xlab=expression("x"[1]), ylab=expression("x"[2]))
for (a in 1:point.count) {
    segments(m[1],m[2],X[a,1],X[a,2], col=2)
}
```

## Total Sum of Squares Calculation

SST is the sum of squares of all points from the centre of the data.

$$
	SST =  {\sum_{i=1}^{N} {\left ( \vec{x}_i - \vec{\bar{x}}\right )}^2}
$$

The mean point is:

$$
	\vec{\bar{x}} = \frac{1}{N}{\sum_{i=1}^{N} \vec{x}_i}
$$
where $N$ is the number of points.

To define SSW and SSB, we need to choose cluster centres.

## Cluster Centres

```{r, echo=FALSE}
c1 = apply(cluster1,2,mean)
c2 = apply(cluster2,2,mean)
c3 = apply(cluster3,2,mean)
c4 = apply(cluster4,2,mean)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X[,1], X[,2], xlab=expression("x"[1]), ylab=expression("x"[2]))
centres = rbind(c1,c2,c3,c4)
points(centres[,1],centres[,2], pch=2, col=2)
```

## Between Cluster Sum of Squares (SSB)

```{r, echo=FALSE}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X[,1], X[,2], xlab=expression("x"[1]), ylab=expression("x"[2]))
points(centres[,1],centres[,2], pch=2, col=2)
segments(m[1],m[2],c1[1],c1[2], col=2)
segments(m[1],m[2],c2[1],c2[2], col=2)
segments(m[1],m[2],c3[1],c3[2], col=2)
segments(m[1],m[2],c4[1],c4[2], col=2)
```

## Within Cluster Sum of Squares (SSW)

```{r, echo=FALSE}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X[,1], X[,2], xlab=expression("x"[1]), ylab=expression("x"[2]))
centres = rbind(c1,c2,c3,c4)
points(centres[,1],centres[,2], pch=2, col=2)
for (a in 1:point.count) {
    segments(centres[cluster.number[a],1],centres[cluster.number[a],2],X[a,1],X[a,2], col=2)
}
```

## Within and Between Sum of Squares Calculations

SSB is the between cluster sum of squares. It is the same as SST, where we move all points to the cluster centre.

$$
	SSB =  {\sum_{i=1, k = C(\vec{x}_i)}^{N} {\left ( \vec{c}_k - \vec{\bar{x}}\right )}^2}
$$
where $C(\vec{x}_i)$ is the cluster number for point $\vec{x}_i$ and $\vec{c}_k$ is the centre of cluster $k$.

SSW is the within cluster sum of squares. It is the same as SST, where
we use the cluster centre instead of the mean.

$$
	SSW =  {\sum_{i=1, k = C(\vec{x}_i)}^{N} {\left ( \vec{x}_i - \vec{c}_k \right )}^2}
$$

## Relationship between SST, SSW and SSB

We want to choose the number of clusters that maximises SSB (large
separation between clusters), and minimises SSW (points in cluster are
close to each other).

For a given set of points, SST is constant (does not depend on the clustering).

For any clustering:
$$
	SST = SSB + SSW
$$

Therefore, if SSB increases, SSW must decrease.

## Comparing SSW and SSB

For the previous set of points, we SSB and SSW:

```{r, echo=FALSE, fig.width=4.4, fig.height=2.8}
wss <- (nrow(X.large)-1)*sum(apply(X.large,2,var))
bss = 0
for (i in 2:15) {
    z = kmeans(X.large, centers=i)
    wss[i] <- z$tot.withinss
    bss[i] <- z$betweenss
}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Sum of squares")
points(1:15, bss, type="b", col=2, pch=2)
legend("right", c("SSW","SSB"), pch=c(1,2), col=c(1,2))
```


## Elbow method

When choosing the number of clusters:

- If we increase the number of clusters, we reduce SSW and increase SSB.
- If we choose to have too many clusters, the clustering becomes worthless.

How do we trade off between minimising the number of clusters and minimising SSB?

### Elbow method

The number of clusters is provided by the clustering where the
reduction of SSW slows (the elbow bend).


## Elbow method example

The number of clusters is given by the position of the *elbow* bend.

```{r, echo=FALSE, fig.width=4.4, fig.height=2.8}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Sum of squares")
points(1:15, bss, type="b", col=2, pch=2)
legend("right", c("SSW","SSB"), pch=c(1,2), col=c(1,2))
```

# Hierarchical Clustering


## Hierarchy of Points

A hierarchy shows the order of its items, usually in the form of a
tree. The higher an item is in the tree, the more important it is.

Hierarchical clustering provides us with a tree of clusters, called a
\alert{dendrogram} that shows the division of clusters.


## Hierarchical Clustering Example

```{r, echo=FALSE}
d = dist(X)
h = hclust(d, "complete")
par(mgp = c(1.5, 0.4, 0), mar=c(0.1,2.5,2.5,0.1), cex=0.8)
plot(h, xlab="x")
```



## Types of Hierarchical Clustering

### Agglomerative Clustering (bottom up)

1. Treat all points as clusters
2. Iteratively merge the most closest clusters, until we are left with one cluster of all points.

### Divisive Clustering (top down)

1. Treat all points as begin in one cluster
2. Iteratively split the cluster with the largest gap, until app points are in their own cluster.

We will not examine Divisive Clustering in this Unit.


## Types of Agglomerative Clustering


Agglomerative clustering requires us to select the two most similar
clusters and merge them.  There are many ways which we can define
cluster similarity, therefore there are many forms of agglomerative
clustering.

We will examine:

- Single Linkage Clustering
- Complete Linkage Clustering
- Group Average Clustering

Each of these methods begin with all points as a cluster, then the
clusters are merged one by one until we have one cluster of all
points.

## Single Linkage Clustering

Single linkage clustering defines the distance between clusters $G$ and $H$ as:
$$
	d_{\text{SL}}(G,H) = \min_{i \in G, j \in H}{d_{ij}}
$$
where $d_{ij}$ is the distance between points $\vec{x}_i$ and $\vec{x}_j$.


## Complete Linkage Clustering

Complete linkage clustering defines the distance between clusters $G$ and $H$ as:
$$
	d_{\text{CL}}(G,H) = \max_{i \in G, j \in H}{d_{ij}}
$$
where $d_{ij}$ is the distance between points $\vec{x}_i$ and $\vec{x}_j$.


## Group Average Clustering

Group average clustering defines the distance between clusters $G$ and $H$ as:
$$
	d_{\text{GA}}(G,H) = \frac{1}{N_G N_H}{\sum_{i \in G} \sum_{j \in H} d_{ij}}
$$
where $d_{ij}$ is the distance between points $\vec{x}_i$ and
$\vec{x}_j$, and $N_G$ and $N_H$ are the number of points in clusters
$G$ and $H$ respectively.

write


## Single Linkage Example

```{r, echo=FALSE}
rpoints = rmvnorm(5, mean = c(5,5), sigma = matrix(c(1,0,0,1),2,2))
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(rpoints, xlab=expression("x"[1]), ylab=expression("x"[2]))
```

## Complete Linkage Problem

### Problem 
Find the hierarchy of clusters from the following plot using complete linkage.

```{r, echo=FALSE, fig.width=4, fig.height=2.2}
rpoints = rmvnorm(5, mean = c(5,5), sigma = matrix(c(1,0,0,1),2,2))
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(rpoints, xlab=expression("x"[1]), ylab=expression("x"[2]))
```


## Single Linkage using a Distance Matrix

We saw that Multidimensional Scaling is a powerful dimension reduction
method because it works off the distance matrix, rather than on the
points themselves.

Single Linkage Clustering also only requires the distance matrix.

To perform clustering:

1. Find the row $i$ and column $j$ associated to the smallest distance.
2. Merge the rows $i$ and $j$ and columns $i$ and $j$ by taking the minimum of each cell.
3. Repeat until we have 1 cluster.

## Single Linkage Distance Matrix Example

The distance between points $x_1$, $x_2$, $x_3$, $x_4$ and $x_5$. 


\begin{center}
%\newcolumntype{B}{c}
\newcolumntype{B}{>{\columncolor{blue!20}}c}
\rowcolors{2}{gray!25}{white}
\begin{tabular}{|B|c|c|c|c|c|}
\hline
\rowcolor{blue!20}
      & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ \\\hline
$x_1$ & 0 & 1 & 7 & 5 & 6 \\
$x_2$ & 1 & 0 & 4 & 8 & 6 \\
$x_3$ & 7 & 4 & 0 & 2 & 8 \\
$x_4$ & 5 & 8 & 2 & 0 & 3 \\
$x_5$ & 6 & 6 & 8 & 3 & 0 \\\hline
\end{tabular}
\end{center}

The table is symmetric since the distance between $x_i$ and $x_j$ is
the same as the distance between $x_j$ and $x_i$.

1. Find the smallest distance (the closest points).
2. Merge the associated points.


## Single Linkage Distance Matrix Problem

### Problem

Find the hierarchy of clusters from the following dissimilarity matrix
using single linkage clustering.

\begin{center}
%\newcolumntype{B}{c}
\newcolumntype{B}{>{\columncolor{blue!20}}c}
\rowcolors{2}{gray!25}{white}
\begin{tabular}{|B|c|c|c|c|c|}
\hline
\rowcolor{blue!20}
      & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ \\\hline
$x_1$ & 0 & 3 & 7 & 8 & 9 \\
$x_2$ & 3 & 0 & 5 & 7 & 6 \\
$x_3$ & 7 & 5 & 0 & 2 & 1 \\
$x_4$ & 8 & 7 & 2 & 0 & 3 \\
$x_5$ & 9 & 6 & 1 & 3 & 0 \\\hline
\end{tabular}
\end{center}


## Summary

From this lecture, we have learnt:

- Clustering allows us to summarise data
- We can cluster data using a clustering algorithm; we examined
  K-means and two forms of Hierarchical clustering.
- The clusters obtained depend on the distance between the objects,
  therefore we must measure the distance appropriately (choose the right metric).
- We must choose the number of clusters for k-means clustering, but we
  are provided all clusters using hierarchical clustering.



## Next Week

Graphs 2: Page Rank

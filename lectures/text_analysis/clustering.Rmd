% Text Mining 2: Clustering
% Laurence A. F. Park
% March 22, 2005

```{r setup, include=FALSE}
opts_chunk$set(dev = 'pdf', fig.cap="", fig.width=4.4, fig.height=3.1)
library("mvtnorm")
```


# Motivation

We have 1000 tweets. What are they about? Can we summarise them?

Clustering allows us to group similar documents or words, allowing us
to identify topics in the document set.

# Introduction to Clustering


# K-means Clustering

## K-means

K-means clustering was designed to be used in a Euclidean space,
meaning it uses the Euclidean distance:
$$
	d(\vec{x}_i,\vec{x}_j) = {\sum_{n=1}^{N} {(x_{in} - x_{jn})}^2} = \norm{\vec{x}_i - \vec{x}_j}_2^2
$$


We want to minimise the within-point scatter over all clusters:
$$
	W(C) = {\sum_{k = 1}^K {\sum_{C(i) = k} {\sum_{C(j) = k} \norm{\vec{x}_i - \vec{x}_j}_2^2}}}
$$
$K$ is the number of clusters, $C(i) = k$ if point $\vec{x}_i$ belongs to cluster $k$, and $W(C)$ is the
sum of the distances between all points in each cluster. By minimising $W(C)$, we obtain tight clusters.


## K-means Algorithm

K-means iterates through two steps, until the centres stabilise (stop moving).

### Compute which object belongs to which cluster

For each object $\vec{x}_i$:
$$
	C(i) = \argmin_{1 \le k \le K}{\norm{\vec{x}_i - \vec{m}_k}}
$$

### Compute the centre of each cluster

For each cluster $k$:
$$
\begin{aligned}
	\vec{m}_k &= \argmin_{\vec{m} \in \mathbb{R}^m}{\sum_{C(i) = k} \norm{\vec{x}_i - \vec{m}}}\\
	    &= \frac{1}{N_k}{\sum_{C(i) = k} \vec{x}_i}
\end{aligned}
$$

## Complete K-means Algorithm

1. Choose $K$ the number of clusters wanted.
2. Assign the cluster centres $\vec{m}_k$ randomly.
3. While the new centres are different to the last centres:
    i. Assign each object to its closest cluster centre.
    ii. Compute the cluster mean from the cluster objects.

## K-means Example: Assign Centres

```{r, echo=FALSE}
cluster1 = rmvnorm(30, mean = c(7,5), sigma = matrix(c(1,0,0,1),2,2))
cluster2 = rmvnorm(30, mean = c(2,12), sigma = matrix(c(1,0,0,1),2,2))
cluster3 = rmvnorm(30, mean = c(15,12), sigma = matrix(c(1,0,0,1),2,2))
cluster4 = rmvnorm(30, mean = c(14,3), sigma = matrix(c(1,0,0,1),2,2))
X.k = rbind(cluster1,cluster2,cluster3,cluster4)
N = nrow(X.k)
clusters = sample(4, N, replace=TRUE)
compute.means = function(X.k, clusters) {
    mean1 = apply(X.k[clusters == 1,],2,mean)
    mean2 = apply(X.k[clusters == 2,],2,mean)
    mean3 = apply(X.k[clusters == 3,],2,mean)
    mean4 = apply(X.k[clusters == 4,],2,mean)
    return(rbind(mean1, mean2, mean3, mean4))
}
means = compute.means(X.k, clusters)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.k[,1], X.k[,2], xlab=expression("x"[1]), ylab=expression("x"[2]))
points(means[,1], means[,2], pch=22, bg=c(1:4))
```



## K-means Example: Assign cluster membership

```{r, echo=FALSE}
cluster.distance = function(x, means) {
    apply(means, 1, point.distance, x)
}
point.distance = function(x, y) {
    return(sum((x - y)^2))
}
cluster.membership = function(X.k, means) {
    return(apply(apply(X.k, 1, cluster.distance, means),2,which.min))
}
clusters = cluster.membership(X.k, means)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.k[,1], X.k[,2], xlab=expression("x"[1]), ylab=expression("x"[2]), col=clusters)
points(means[,1], means[,2], pch=22, bg=c(1:4))
```

## K-means Example: Recompute cluster centres

```{r, echo=FALSE}
means = compute.means(X.k, clusters)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.k[,1], X.k[,2], xlab=expression("x"[1]), ylab=expression("x"[2]), col=clusters)
points(means[,1], means[,2], pch=22, bg=c(1:4))
```

## K-means Example: Reassign cluster membership

```{r, echo=FALSE}
clusters = cluster.membership(X.k, means)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.k[,1], X.k[,2], xlab=expression("x"[1]), ylab=expression("x"[2]), col=clusters)
points(means[,1], means[,2], pch=22, bg=c(1:4))
```

## K-means Example: Recompute cluster centres

```{r, echo=FALSE}
means = compute.means(X.k, clusters)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.k[,1], X.k[,2], xlab=expression("x"[1]), ylab=expression("x"[2]), col=clusters)
points(means[,1], means[,2], pch=22, bg=c(1:4))
```

## K-means Problem

For the following points and cluster centres:

1. Compute the cluster membership of each point.
2. Recompute the cluster centres.

Insert data here.

## Problems with K-means

- K means requires the cluster centres to allocated before the
  iteration begins. If we choose random assignment, we may obtain
  different cluster results with each run.

- K-means requires us to set the number of clusters before the
  algorithm is run, but we usually don't know how many clusters there
  are.

- K-means uses Euclidean distance, which may not be appropriate for
  us.


## Spherical K-means Algorithm

We may want to use K-means with the Cosine similarity, rather than
Euclidean distance (remember that Cosine similarity measures the
Cosine of the angle between the vectors). If we normalise the points
($\vec{z}_i = \vec{x}_i/\norm{\vec{x}_i}$):

### Compute which object belongs to which cluster

For each object $\vec{x}_i$:
$$
	C(i) = \argmin_{1 \le k \le K}{\frac{\vec{x}_i \cdot \vec{m}_k}{\norm{\vec{x}_i}\norm{\vec{m}_k}}}
	= \argmin_{1 \le k \le K}{\vec{z}_i \cdot \vec{m}_k}
$$

### Compute the centre of each cluster

For each cluster $k$:
$$
	\vec{m}_k = \frac{\sum_{C(i) = k} \vec{z}_i}{\norm{\sum_{C(i) = k} \vec{z}_i}}
$$


## Sample Data Set

How many clusters are there?

```{r, echo=FALSE}
cluster1 = rmvnorm(30, mean = c(7,5), sigma = matrix(c(1,0,0,1),2,2))
cluster2 = rmvnorm(30, mean = c(2,12), sigma = matrix(c(1,0,0,1),2,2))
cluster3 = rmvnorm(30, mean = c(15,12), sigma = matrix(c(1,0,0,1),2,2))
cluster4 = rmvnorm(30, mean = c(14,3), sigma = matrix(c(1,0,0,1),2,2))
X.large = rbind(cluster1,cluster2,cluster3,cluster4)
#par(mar = (c(3.5, 3.5, 4, 2) + 0.1))
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X.large[,1], X.large[,2], xlab=expression("x"[1]), ylab=expression("x"[2]),)
```


## Total Sum of Squares (SST)

```{r, echo=FALSE}
cluster1 = rmvnorm(5, mean = c(5,5), sigma = matrix(c(1,0,0,1),2,2))
cluster2 = rmvnorm(5, mean = c(2,12), sigma = matrix(c(1,0,0,1),2,2))
cluster3 = rmvnorm(5, mean = c(15,9), sigma = matrix(c(1,0,0,1),2,2))
cluster4 = rmvnorm(5, mean = c(14,3), sigma = matrix(c(1,0,0,1),2,2))
cluster.number = c(rep(1,5),rep(2,5),rep(3,5),rep(4,5))
X = rbind(cluster1,cluster2,cluster3,cluster4)
m = apply(X,2,mean)
point.count = nrow(X)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X[,1], X[,2], xlab=expression("x"[1]), ylab=expression("x"[2]))
for (a in 1:point.count) {
    segments(m[1],m[2],X[a,1],X[a,2], col=2)
}
```

## Cluster Centres

```{r, echo=FALSE}
c1 = apply(cluster1,2,mean)
c2 = apply(cluster2,2,mean)
c3 = apply(cluster3,2,mean)
c4 = apply(cluster4,2,mean)
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X[,1], X[,2], xlab=expression("x"[1]), ylab=expression("x"[2]))
centres = rbind(c1,c2,c3,c4)
points(centres[,1],centres[,2], pch=2, col=2)
```

## Between Cluster Sum of Squares (SSB)

```{r, echo=FALSE}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X[,1], X[,2], xlab=expression("x"[1]), ylab=expression("x"[2]))
points(centres[,1],centres[,2], pch=2, col=2)
segments(m[1],m[2],c1[1],c1[2], col=2)
segments(m[1],m[2],c2[1],c2[2], col=2)
segments(m[1],m[2],c3[1],c3[2], col=2)
segments(m[1],m[2],c4[1],c4[2], col=2)
```

## Within Cluster Sum of Squares (SSW)

```{r, echo=FALSE}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(X[,1], X[,2], xlab=expression("x"[1]), ylab=expression("x"[2]))
centres = rbind(c1,c2,c3,c4)
points(centres[,1],centres[,2], pch=2, col=2)
for (a in 1:point.count) {
    segments(centres[cluster.number[a],1],centres[cluster.number[a],2],X[a,1],X[a,2], col=2)
}
```


For any clustering:
$$
	SST = SSB + SSW
$$

A clustering should maximise SSB (the between cluster distance) and
minimise SSW (the within cluster distance).


## Elbow method

When choosing the number of clusters:

- If we increase the number of clusters, we reduce SSW and increase SSB.
- If we choose to have too many clusters, the clustering becomes worthless.

How do we trade off between minimising the number of clusters and minimising SSB?

The number of clusters is provided by the clustering where the reduction of SSE slows.

## Elbow method example

The number of clusters is given by the position of the *elbow* bend.

```{r, echo=FALSE}
wss <- (nrow(X.large)-1)*sum(apply(X.large,2,var))
bss = 0
for (i in 2:15) {
    z = kmeans(X.large, centers=i)
    wss[i] <- z$tot.withinss
    bss[i] <- z$betweenss
}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Sum of squares")
points(1:15, bss, type="b", col=2, pch=2)
legend("right", c("SSW","SSB"), pch=c(1,2), col=c(1,2))
```

# Hierarchical Clustering

## Hierarchical Clustering Example

```{r, echo=FALSE}
d = dist(X)
h = hclust(d, "complete")
par(mgp = c(1.5, 0.4, 0), mar=c(0.1,2.5,2.5,0.1), cex=0.8)
plot(h, xlab="x")
```



## Types of Hierarchical Clustering

### Agglomerative Clustering (bottom up)

1. Treat all points as clusters
2. Iteratively merge the most closest clusters, until we are left with one cluster of all points.

### Divisive Clustering (top down)

1. Treat all points as begin in one cluster
2. Iteratively split the cluster with the largest gap, until app points are in their own cluster.

We will not examine Divisive Clustering in this Unit.


## Types of Agglomerative Clustering


Agglomerative clustering requires us to select the two most similar
clusters and merge them.  There are many ways which we can define
cluster similarity, therefore there are many forms of agglomerative
clustering.

We will examine:

- Single Linkage Clustering
- Complete Linkage Clustering
- Group Average Clustering

Each of these methods begin with all points as a cluster, then the
clusters are merged one by one until we have one cluster of all
points.

## Single Linkage Clustering

Single linkage clustering defines the distance between clusters $G$ and $H$ as:
$$
	d_{\text{SL}}(G,H) = \min_{i \in G, j \in H}{d_{ij}}
$$
where $d_{ij}$ is the distance between points $\vec{x}_i$ and $\vec{x}_j$.


## Complete Linkage Clustering

Complete linkage clustering defines the distance between clusters $G$ and $H$ as:
$$
	d_{\text{CL}}(G,H) = \max_{i \in G, j \in H}{d_{ij}}
$$
where $d_{ij}$ is the distance between points $\vec{x}_i$ and $\vec{x}_j$.


## Group Average Clustering

Group average clustering defines the distance between clusters $G$ and $H$ as:
$$
	d_{\text{GA}}(G,H) = \frac{1}{N_G N_H}{\sum_{i \in G} \sum_{j \in H} d_{ij}}
$$
where $d_{ij}$ is the distance between points $\vec{x}_i$ and
$\vec{x}_j$, and $N_G$ and $N_H$ are the number of points in clusters
$G$ and $H$ respectively.

write


## Single Linkage Example

```{r, echo=FALSE}
rpoints = rmvnorm(5, mean = c(5,5), sigma = matrix(c(1,0,0,1),2,2))
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(rpoints, xlab=expression("x"[1]), ylab=expression("x"[2]))
```

## Single Linkage Example

```{r, echo=FALSE}
par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(rpoints, xlab=expression("x"[1]), ylab=expression("x"[2]))
point.count = nrow(rpoints)
for (a in (1:(point.count-1))) {
    for (b in ((a+1):point.count)) {
        segments(rpoints[a,1],rpoints[a,2],rpoints[b,1],rpoints[b,2], col=2, lty=3)
    }
}
```

## Single Linkage Example

```{r, echo=FALSE}
d = dist(rpoints)
D = as.matrix(d)
# take diag out of calculation
diag(D) = max(D)+1
min.pos = arrayInd(which.min(D), dim(D))
clusters = 1:5

par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(rpoints, xlab=expression("x"[1]), ylab=expression("x"[2]))
point.count = nrow(rpoints)
for (a in (1:(point.count-1))) {
    for (b in ((a+1):point.count)) {
        segments(rpoints[a,1],rpoints[a,2],rpoints[b,1],rpoints[b,2], col=2, lty=3)
    }
}
segments(rpoints[min.pos[1],1],rpoints[min.pos[1],2],rpoints[min.pos[2],1],rpoints[min.pos[2],2], col=2, lty=1)
clusters[min.pos[2]] = clusters[min.pos[1]]
```

## Single Linkage Example

```{r, echo=FALSE}
unique.clusters = unique(clusters)

par(mgp = c(1.5, 0.4, 0), mar=c(2.5,2.5,0.1,0.1), cex=0.8)
plot(rpoints, xlab=expression("x"[1]), ylab=expression("x"[2]))
point.count = nrow(rpoints)

edist = function(x,y) {
    return(sqrt(sum((x - y)^2)))
}
sing.link.dist = function(y, z) {
    return(apply(z, 1,  edist, y))
}
all.sing.link.dist = function(y, z) {
    return(apply(y, 1, sing.link.dist, z))
}



closest.pos = function(clusters, a, b, rpoints) {
    y = rpoints[which(clusters == a),, drop=FALSE]
    y.pos = which(clusters == a)
    z = rpoints[which(clusters == b),, drop=FALSE]
    z.pos = which(clusters == b)
    d = as.matrix(all.sing.link.dist(z,y))
    min.pos = arrayInd(which.min(d), dim(d))
    return(c(y.pos[min.pos[1]],z.pos[min.pos[2]]))
}


for (a in unique.clusters[1:(length(unique.clusters)-1)]) {
    for (b in unique.clusters[2:length(unique.clusters)]) {
    	pos = closest.pos(clusters, a, b, rpoints)
        segments(rpoints[pos[1],1],rpoints[pos[1],2],rpoints[pos[2],1],rpoints[pos[2],2], col=2, lty=3)
    }
}
segments(rpoints[min.pos[1],1],rpoints[min.pos[1],2],rpoints[min.pos[2],1],rpoints[min.pos[2],2], col=2, lty=1)
```

## Single Linkage using a Distance Matrix

We saw that Multidimensional Scaling is a powerful dimension reduction
method because it works off the distance matrix, rather than on the
points themselves.

Single Linkage Clustering also only requires the distance matrix.

To perform clustering:

1. Find the row $i$ and column $j$ associated to the smallest distance.
2. Merge the rows $i$ and $j$ and columns $i$ and $j$ by taking the minimum of each cell.
3. Repeat until we have 1 cluster.

## Single Linkage Distance Matrix Example

The distance between points $x_1$, $x_2$, $x_3$, $x_4$ and $x_5$. 


\begin{center}
\only<1-6>{
\newcolumntype{A}{c}
\newcolumntype{A}{>{\columncolor{blue!20}}c}
%\rowcolors{2}{gray!25}{white}
\begin{tabular}{|A|c|c|c|c|c|}
\hline
\rowcolor{blue!20}
      & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ \\\hline
\only<1>{%
$x_1$ & 0 & \alert{1} & 7 & 5 & 3 \\
\hline
$x_2$ & \alert{1} & 0 & 4 & 2 & 6 \\
}%
\only<2>{%
$x_1$ & \multirow{2}{*}{0} & \alert{1} & 7 & 5 & 3 \\
$x_2$ &  & 0 & 4 & 2 & 6 \\
}%
\only<3>{%
$x_1$ & \multirow{2}{*}{0} & \multirow{2}{*}{0} & 7 & 5 & 3 \\
$x_2$ &  &  & 4 & 2 & 6 \\
}%
\only<4>{%
$x_1$ & \multirow{2}{*}{0} & \multirow{2}{*}{0} & \multirow{2}{*}{4} & 5 & 3 \\
$x_2$ &  &  &  & 2 & 6 \\
}%
\only<5>{%
$x_1$ & \multirow{2}{*}{0} & \multirow{2}{*}{0} & \multirow{2}{*}{4} & \multirow{2}{*}{2} & 3 \\
$x_2$ &  &  &  &  & 6 \\
}%
\only<6>{%
$x_1$ & \multirow{2}{*}{0} & \multirow{2}{*}{0} & \multirow{2}{*}{4} & \multirow{2}{*}{2} & \multirow{2}{*}{3} \\
$x_2$ &  &  &  &  &  \\
}%
%\hline
$x_3$ & 7 & 4 & 0 & 9 & 8 \\\hline
$x_4$ & 5 & 5 & 9 & 0 & 4 \\\hline
$x_5$ & 3 & 6 & 8 & 4 & 0 \\\hline
\end{tabular}
}
\only<7>{
\newcolumntype{A}{c}
\newcolumntype{A}{>{\columncolor{blue!20}}c}
%\rowcolors{2}{gray!25}{white}
\begin{tabular}{|A|c|c|c|c|c|}
\hline
\rowcolor{blue!20}
      & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ \\\hline
$x_1$ & \multirow{2}{*}{0} & \alert{1} & 7 & 5 & 3 \\
$x_2$ &  & 0 & 4 & 2 & 6 \\\hline
$x_3$ & 7 & 4 & 0 & 9 & 8 \\\hline
$x_4$ & 5 & 5 & 9 & 0 & 4 \\\hline
$x_5$ & 3 & 6 & 8 & 4 & 0 \\\hline
\end{tabular}
}
\only<8>{
\newcolumntype{A}{c}
\newcolumntype{A}{>{\columncolor{blue!20}}c}
%\rowcolors{2}{gray!25}{white}
\begin{tabular}{|A|c|c|c|c|}
\hline
\rowcolor{blue!20}
      & $x_1$,$x_2$ & $x_3$ & $x_4$ & $x_5$ \\\hline
$x_1$,$x_2$ & 0 & 4 & 2 & 3 \\\hline
$x_3$       & 4 & 0 & 9 & 8 \\\hline
$x_4$       & 5 & 9 & 0 & 4 \\\hline
$x_5$       & 3 & 8 & 4 & 0 \\\hline
\end{tabular}
}
\end{center}

The table is symmetric since the distance between $x_i$ and $x_j$ is
the same as the distance between $x_j$ and $x_i$.

1. Find the smallest distance (the closest points).
2. Merge the associated points.




# Choosing the number of clusters


## Summary

From this lecture, we have learnt:

- Clustering allows us to summarise data
- We can cluster data using a clustering algorithm; we examined
  K-means and two forms of Hierarchical clustering.
- The clusters obtained depend on the distance between the objects,
  therefore we must measure the distance appropriately (choose the right metric).
- We must choose the number of clusters for k-means clustering, but we
  are provided all clusters using hierarchical clustering.



## Next Week

Graphs 2: Page Rank


# Evaluating Results

## Ranking documents

Given a document model, we can compute how similar all documents are
to a particular document, or how similar all documents are to a
query. Either way, the result is a ranked list of documents from best
matching to worst matching.

\begin{center}
\begin{tabular}{lcc}
\toprule
  Document ID & Score & Rank \\
\midrule
  D1012123 & 4.56 & 1 \\
  D2342923 & 4.23 & 2 \\
  D2342233 & 3.78 & 3 \\
  D7453232 & 3.66 & 4 \\
  \vdots & \vdots & \vdots \\
  D6454644 & -4.01 & 983453 \\
\bottomrule
\end{tabular}
\end{center}

The document score is used to provide the rank. Once we have the rank,
we can ignore the score. Google presents the user with the top ten
results, with the option to view the next ten.

## Comparing results for difference document models

If we are given two document models, we can compute two different
ranked lists for the same problem (query). We can use these ranked
lists to evaluate the accuracy of each document model.

To evaluate a document model, we need:

- the truth (the correct ranking)
- an evaluation function (to compare the computed ranking to the correct ranking).


## Obtaining the truth

The ranking of an entire document set for a given query is a
subjective process. Given only 10 documents, there are $10! =
3,628,800$ different possible rankings. If two documents are similar,
it is difficult to say which should be ranked higher than the other.

Rather than providing a ranking as the truth, we can instead provide
the set of documents that are relevant to the query, meaning that the
remaining documents are irrelevant.

A perfect ranking would place all relevant documents higher on the
list than all irrelevant documents. But how do we score ranking that
are not perfect?

### Which ranking is better

\begin{columns}
\begin{column}{0.45\textwidth}
The best ranking is:
\begin{itemize}
\item 1 1 1 1 0 0 0 0
\end{itemize}
\end{column}
\begin{column}{0.45\textwidth}
Which is the better of these two:
\begin{itemize}
\item 1 0 0 0 1 1 1 0
\item 0 1 1 0 1 1 0 0
\end{itemize}
\end{column}
\end{columns}

## Precision at $k$

A simple measure of accuracy is to count the number of relevant
documents in the top $k$ ranked documents.

The precision at 5 for the following ranking is 2:
\begin{center}
	1 0 0 1 0 1 1 0 1 1
\end{center}

### Problem

Calculate the precision at 5 for these rankings:

- 0 0 1 1 1 0 1 1 0 0
- 1 0 0 0 0 0 0 1 0 1
- 1 1 1 1 1 1 1 1 0 1

## Reciprocal Rank

In some cases, we are interested on returning one relevant document,
rather then a number of them. For this case, the position of the
relevant document is important.

The reciprocal rank is the reciprocal of the rank of the first
relevant document. If the first relevant document appears at rank $x$,
the reciprocal rank is $1/x$.

### Problem

Calculate the reciprocal rank for the following rankings:

- 0 0 1 1 1 0 1 1 0 0
- 1 0 0 0 0 0 0 1 0 1
- 1 1 1 1 1 1 1 1 0 1


## Average Precision

If we want to measure how well we ranked all documents, we use Average
Precision. This is the average of the precision at each relevant
document:

$$
	\text{AP} = \frac{1}{R}{\sum_{i = 1}^{R} \frac{i}{r_i}}
$$
where $r_i$ is the rank is the $i$th top ranked relevant document.

### Example

Given the ranked list with relevant documents in the positions: 

1 1 0 0 0 1

$AP = (1/1 + 2/2 + 3/6)/3 = 0.833$









## Document model qualities

In each cases, we need to be able to compare documents to documents,
documents to queries, or words to words, so we need to define a
document and word similarity (or dissimilarity) function:
$$
   s_{i,j} = S(d_i, d_j) 
$$
where $s_{i,j}$ is large if $d_i$ and $d_j$ are similar, and 
$s_{i,j}$ is small if $d_i$ and $d_j$ are dissimilar.

Before we can define a similarity function, we need to define how the
document will be represented. Hence we need to define a mathematical
model for a document, also known as a document model.


# Weighting Terms


## Term weights

For a given document collection, we can compute which words are the
most descriptive of the set.

### Requirements for a term weight

- If a term appears in all documents, then is does not provide us with
any information.  
- If a term appears in only one document, then it is
very representative of the difference between that document and the
rest.
